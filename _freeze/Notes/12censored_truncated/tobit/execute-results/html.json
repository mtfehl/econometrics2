{
  "hash": "c2367287a7896cc675c7cb80dc665b6f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Censored/Truncated Regression\"\nauthor: \"Michael Fehl\"\ndescription: \"Dealing with Unobserved Data: Censored, Truncated, and the Tobit Model\"\ndate: 02-02-2026\nimage: /images/tobit.png\nclass-notes: true\nformat:\n  html:\n    embed-resources: true\n---\n\n# Theory\n\n## 1. Dealing with Unobserved Data\n\nWhen working with panel or cross-sectional data, we often encounter situations where data is incompletely observed. It is crucial to distinguish between two types:\n\n### A. Censored vs. Truncated Data\n\n1.  **Censored Data:** Information is lost on the *dependent variable* ($y$), but we have full information on the regressors ($x$) for the entire sample.\n    * *Example:* Income surveys where high earners are recorded simply as \"$100k+\". We know their age, education, etc., but their exact income is \"right-censored.\"\n    * *Key Feature:* We have a representative sample of the population, but the outcome value is \"stuck\" at a threshold for some.\n\n2.  **Truncated Data:** We do not have the full sample. Data is lost on both the dependent variable and the regressors because the observation rule depends on $y$.\n    * *Example:* Studying determinants of poverty but only sampling people below the poverty line. We completely lack data (both $y$ and $x$) for anyone above the line.\n    * *Key Feature:* The sample is a non-representative sub-sample of the population.\n\n---\n\n## 2. The Latent Variable Model\n\nTo model these processes, we assume there is an underlying \"Latent Variable\" $y^*$ that satisfies the classical linear assumptions:\n\n$$\ny_i^* = x_i' \\beta + \\varepsilon_i\n$$\n\nHowever, $y^*$ is not perfectly observed.\n\n### Observation Rules\n\n**1. Censoring from Below (Left-Censoring)**\n\nWe observe $y_i$ based on a threshold $L$:\n\n$$\ny_i = \\begin{cases} \ny_i^* & \\text{if } y_i^* > L \\\\\nL & \\text{if } y_i^* \\leq L \n\\end{cases}\n$$\n\n*Result:* A \"spike\" (mass point) in the distribution of $y$ at $L$.\n\n**2. Censoring from Above (Right-Censoring)**\n\n$$\ny_i = \\begin{cases} \ny_i^* & \\text{if } y_i^* < U \\\\\nU & \\text{if } y_i^* \\geq U \n\\end{cases}\n$$\n\n**3. Truncation from Below**\n\nWe only observe unit $i$ if $y_i^* > L$.\n\n$$\n\\text{Sample Rule: Keep } i \\text{ if } y_i^* > L\n$$\n\n*Result:* The distribution is \"cut off.\" The area under the curve must sum to 1, so the remaining probability density is scaled up.\n\n## 3. Why OLS Fails (Bias Analysis)\n\nIf we run a standard OLS regression ($y$ on $x$) using censored data, our estimates will be biased.\n\n### Visualizing the Bias\nAt low values of $x$, many observations hit the lower bound $L$. This flattens the slope of the observed relationship relative to the true latent relationship.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](tobit_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n### The Expectation Problem\n\nThe expected value of the observed data is:\n\n$$\nE[y|x] = P(y^* \\le L) \\cdot L + P(y^* > L) \\cdot E[y^* | y^* > L, x]\n$$\n\nThis relationship is non-linear.\n\n  * At high $x$, $E[y|x] \\approx E[y^*|x]$ (convergence to true slope).\n  * At low $x$, data piles up at $L$, dragging the expectation line flat.\n  * **Consequence:** OLS will generally **underestimate** the slope coefficient $\\beta$.\n\n---\n\n## 4. Estimation Strategies\n\nSince OLS is inconsistent, we must use **Maximum Likelihood Estimation (MLE)**. We need to make parametric assumptions (usually Normality) about the error term: $\\varepsilon \\sim N(0, \\sigma^2)$.\n\n### Case A: Censored Data (Left-Censored at $L$)\n\nThe density of observed $y$ is a mixture of a continuous part and a discrete probability mass.\n\n1.  **Continuous part ($y_i > L$):** We observe the actual value. The likelihood contribution is the PDF: $f^*(y_i | x_i)$.\n2.  **Censored part ($y_i = L$):** We know only that $y^* \\le L$. The likelihood contribution is the probability (CDF): $F^*(L | x_i)$.\n\nWe define a dummy indicator $d_i$:\n\n$$\nd_i = \\begin{cases} \n1 & \\text{if } y_i > L \\quad (\\text{Uncensored}) \\\\\n0 & \\text{if } y_i = L \\quad (\\text{Censored})\n\\end{cases}\n$$\n\n**The Log-Likelihood Function:**\n\n$$\n\\ell(\\beta, \\sigma^2) = \\sum_{i=1}^N \\left[ d_i \\ln \\left( \\frac{1}{\\sigma} \\Phi\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) \\right) + (1-d_i) \\ln \\Phi\\left(\\frac{L - x_i'\\beta}{\\sigma}\\right) \\right]\n$$\n\n### Case B: Truncated Data (Truncated from below at L)\n\nHere, we do not observe *anyone* with $y^* \\le L$. We must re-normalize the probability density so it integrates to 1 over the range $(L, \\infty)$.\n\n$$\nf(y_i | y_i > L, x_i) = \\frac{f^*(y_i|x_i)}{P(y_i > L | x_i)} = \\frac{\\frac{1}{\\sigma}\\phi(\\frac{y_i - x_i'\\beta}{\\sigma})}{1 - \\Phi(\\frac{L - x_i'\\beta}{\\sigma})}\n$$\n\n**The Log-Likelihood Function:**\n\n$$\n\\ell = \\sum_{i=1}^N \\left[ \\ln \\left( \\frac{1}{\\sigma}\\Phi(\\cdot) \\right) - \\ln \\left( 1 - \\Phi\\left(\\frac{L - x_i'\\beta}{\\sigma}\\right) \\right) \\right]\n$$\n\n**Intuition:** In the truncated case, the denominator $1 - \\Phi(\\cdot)$ essentially tells the model: \"Hey, we are missing the left tail of the bell curve, so inflate the probabilities of the data we *do* see to account for that missing mass.\"\n\n## 5. The Tobit Model (Censored Normal Regression)\n\nThe Tobit model is the specific and most common case of censored regression where the **threshold is zero ($L=0$)** and censoring is from below.\n\n### Setup\n\n  * **Latent Variable:** $y_i^* = x_i'\\beta + \\varepsilon_i$, with $\\varepsilon_i \\sim N(0, \\sigma^2)$.\n  * **Observation Rule:**\n\n$$\ny_i = \\max(0, y_i^*)\n$$\n\n### Application: Labor Supply (Hours Worked)\n\nA classic example is hours worked ($H_i$).\n\n  * $H_i^*$: Desired hours (latent utility). A person might desire \"-5 hours\" of work (meaning they need a high wage just to start working).\n  * $H_i$: Actual observed hours. Since you can't work negative hours, we observe 0.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](tobit_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n### Deriving the Likelihood\n\nWe split the sample into those working ($H_i > 0$) and those not working ($H_i = 0$).\n\n1.  **For $H_i > 0$ (Uncensored):**\n    Contribution is the density of the normal distribution:\n    \n$$\n\\frac{1}{\\sigma} \\Phi\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right)\n$$\n\n2.  **For $H_i = 0$ (Censored):**\n    Contribution is the probability that latent utility is negative:\n    \n$$\n\\mathbb{P}(y_i^* \\le 0) = \\mathbb{P}\\left(\\frac{y_i^* - x_i'\\beta}{\\sigma} \\le \\frac{-x_i'\\beta}{\\sigma}\\right) = \\Phi\\left(-\\frac{x_i'\\beta}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\n$$\n\n**Combined Log-Likelihood:**\n\n$$\n\\ell(\\beta, \\sigma) = \\sum_{y_i = 0} \\ln \\left[ 1 - \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right) \\right] + \\sum_{y_i > 0} \\ln \\left[ \\frac{1}{\\sigma} \\phi\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) \\right]\n$$\n\n### Identification & Interpretation\n\n  * Ideally, we estimate $\\beta$ and $\\sigma^2$ separately.\n  * However, if there is high multicollinearity or little variation in the censored portion, it can be difficult to disentangle $\\beta$ from $\\sigma$.\n  * **Homoskedasticity Assumption:** Crucial for Tobit. If $\\text{Var}(\\varepsilon)$ varies with $x$, the Tobit estimates are inconsistent (unlike OLS, where heteroskedasticity only affects standard errors).\n\n### Marginal Effects (ME)\n\nIn Tobit, there are two different marginal effects of interest:\n\n1.  **Effect on the Latent Variable:**\n\n$$\n\\frac{\\partial E[y^*|x]}{\\partial x_k} = \\beta_k\n$$\n    \n*Interpretation:* How a change in $x$ changes \"desired\" hours.\n\n2.  **Effect on the Observed Variable:**\n\n$$\n\\frac{\\partial E[y|x]}{\\partial x_k} = \\beta_k \\cdot \\Phi\\left(\\frac{x'\\beta}{\\sigma}\\right)\n$$\n   \n*Interpretation:* How a change in $x$ changes *actual* hours worked.\n    \n**Note:** The effect on actual hours is always smaller than the effect on desired hours because $0 \\le \\Phi(\\cdot) \\le 1$. This dampening factor $\\Phi$ represents the probability of being uncensored (actually working).\n    \n",
    "supporting": [
      "tobit_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}