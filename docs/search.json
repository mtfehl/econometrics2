[
  {
    "objectID": "Projects/Project01/pj01.html",
    "href": "Projects/Project01/pj01.html",
    "title": "Project 01",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Problem_Sets/PS02/ps02.html",
    "href": "Problem_Sets/PS02/ps02.html",
    "title": "Problem Set 02",
    "section": "",
    "text": "Question 1:\n\n\nQuestion 2:\n\nConsider the following model: \\[y_{it} = \\alpha + x_{it}\\beta + z_{it}\\gamma + f_i + u_{it} \\quad \\text{for } i=1,\\dots,N \\text{ and } t=1,\\dots,T,\\] where \\(x_{it}\\) and \\(z_{it}\\) are scalars, \\(f_i\\) is a permanent unobserved effect and the error term \\(u_{it}\\) is homoscedastic and serially uncorrelated. Furthermore, assume that \\(x_{it}\\) is strictly exogenous: \\[E(u_{it}|x_{i1}, \\dots, x_{iT}, f_i) = 0. \\tag{1}\\]\n\n\nSuppose the only thing we can safely assume is that the orthogonality condition (1) holds and you take first differences to estimate the model. For different assumptions regarding the exogeneity of \\(z_{it}\\) and the relationship between \\(z_{it}\\) and \\(f_i\\), state the properties of your estimator (consistency and efficiency) when OLS is used to estimate \\(\\beta\\) and \\(\\gamma\\) in the first differences model?\n\nNote that the FD model takes the form: \\(\\Delta y_i = \\Delta x_i' \\beta + \\Delta z_i' \\gamma + \\Delta \\mu_i\\)\nCase 1.1: \\(E[f_i \\mid z_i, x_i] = 0\\) & Strict exo\nConsistent\nNot efficient: random effects would be efficient model here\n$$\n$$\nCase 1.2: \\(E[f_i \\mid z_i, x_i] \\neq 0\\) & Strict exo\nConsistent\nNot efficient: fixed effects would be efficient (serially correlated errors); for FD random walk is more efficient model\n\nNow suppose \\(T=5\\) and the additional assumption holds: \\[E(u_{it}|z_{i1}, \\dots, z_{it-1}, f_i) = 0.\\]\n\nCase 2.1: \\(E[f_i \\mid z_i, x_i] = 0\\) & weak exo\nInconsistent: $$ \\[\\begin{align}\n  E[\\Delta z_{it} \\Delta \\mu_{it}] &= \\underbrace{E[z_{it} \\mu_{it}]}_{=0} - \\underbrace{E[z_{it-1} \\mu_{it}]}_{\\neq 0} \\\\\n  &- \\underbrace{E[z_{it-1} \\mu_{it}]}_{=0} - \\underbrace{E[z_{it-1} \\mu_{it-1}]}_{=0}\n  \n\\end{align}\\] $$\nHence, when we try to apply asymptotic properties, it does not converge in probability to zero; rather, it converges to a non-zero value. Thus, it is not consistent.\nCase 2.2: \\(E[f_i \\mid z_i, x_i] \\neq 0\\) & weak exo\nInconsistent:\n\nNow suppose \\(T=5\\) and the additional assumption holds: \\(E(\\mu_{it}|z_{i1}, ..., z_{it−1}, f_i) = 0\\). (NOT weak exogenity; contemperous error terms)\n\n(i). How would you now estimate \\(\\beta\\) and \\(\\gamma\\) efficiently?\nWe need instruments to estimate our FD model efficiently; specifically, an instrument for \\(\\Delta z_{it}\\)\nNote that our error assumption does NOT include exogenity of the error term from the same time period of z. I.e., we need to make sure that our instrument does not include correlations between \\(\\mu_{it}\\) and \\(z_{it}\\)\nFor example, let’s explore AH (use \\(z_{it-1}\\) as an IV):\n\\[E[z_{it-1} \\Delta \\mu_{it}] = E[z_{it-1} \\mu_{it} - z_{it-1} \\mu_{it-1}] \\neq 0\\]\nDoes not work – need to go further back in time. Let’s try \\(z_{it-2}\\) as an IV\n\\[E[z_{it-2} \\Delta \\mu_{it}] = E[z_{it-2} \\mu_{it} - z_{it-2} \\mu_{it-1}] = 0\\]\nSo, as long as we take any IV that is at least \\(t-2\\) periods prior (and that the regressor is still correlated with \\(x_{it}\\))\nSince we have five periods; the first time period we can use is \\(t=3\\); we cannot instrument anything earlier (\\(t=3, t=4, t=5\\)). In other words, the first regressors we can include in the model are from t=3 to t=5 …? corresponding to (\\(\\Delta z_{i3}, \\Delta z_{i4}, \\Delta z_{i5}\\))\nCase 1: \\(\\Delta z_{i3}\\)\n\\(E[z_{i1} \\Delta \\mu_{i3}] = 0\\)\nCase 2: \\(\\Delta z_{i4}\\)\n\\(E[z_{i1} \\Delta \\mu_{i4}] = 0\\) \\(E[z_{i2} \\Delta \\mu_{i4}] = 0\\)\nCase 3: \\(\\Delta z_{i5}\\)\n\\(E[z_{i1} \\Delta \\mu_{i5}] = 0\\) \\(E[z_{i2} \\Delta \\mu_{i5}] = 0\\) \\(E[z_{i3} \\Delta \\mu_{i5}] = 0\\)\n** These are all the moment conditions we can get; assuming we want to use this class of estimators to find all the orthogonality conditions in our model to instrument \\(\\Delta z_{it}\\)\nNow the question is to how to combine all of these into one big instrument matrix that we then put into our GMM estimator. The instrument matrix that we select is the following:\n\\[\nz_i =\n\\begin{bmatrix}\n  z_{i1} &0 & 0& 0&0 &0 & \\Delta x_{i3} \\\\\n  0 & z_{i1} & z_{i1} & 0 &0 & 0& \\Delta x_{i4} \\\\\n  0 & 0 & 0 & z_{i1} & z_{i1} & z_{i1} & \\Delta x_{i5}\n\\end{bmatrix}\n\\]\nSuch that we satisfy the condition:\n$$ E[z_i’ _i] = E(\n\\[\\begin{bmatrix}\n  z_{i1} &0 & 0& 0&0 &0 & \\Delta x_{i3} \\\\\n  0 & z_{i1} & z_{i1} & 0 &0 & 0& \\Delta x_{i4} \\\\\n  0 & 0 & 0 & z_{i1} & z_{i1} & z_{i1} & \\Delta x_{i5}\n\\end{bmatrix}\\]\n’\n\\[\\begin{bmatrix}\n  \\Delta \\mu_{i3} \\\\\n  \\Delta \\mu_{i4} \\\\\n  \\Delta \\mu_{i4}\n\\end{bmatrix}\\]\n) = 0 $$ Which leads us to the decision to use AB estimator as we care about efficiency. We want each condition above to have its own moment condition; i.e., we use every orthogonality condition available in the model.\nNow we can define \\(k_i\\) as the following:\n\\[\nk_i =\n\\begin{bmatrix}\n  \\Delta x_{i3} & \\Delta z_{i3} \\\\\n  \\Delta x_{i4} & \\Delta z_{i4} \\\\\n  \\Delta x_{i5} & \\Delta z_{i5}\n\\end{bmatrix}\n\\]\nWhich we plug into our standard GMM estimator form:\n$$ _{GMM} = ( (k_i’z_i ) ^{-1} (z_i’zk_i )^{-1} (k_i’z_i ) ^{-1} (z_i’y_i )\n) $$\nNote that the weighting matrix form that gives us the min. variance takes the form:\n\\[\nplim \\hat{ \\Omega} = Var(z_i \\Delta \\mu_i)\n\\]\nTo achieve this, it depends on our error term assumption. If we assume homoskedasticity, we can derive the variance directly since we assume the functional form of the errors. If we have heteroskedasticity, we cannot do that (since we lack a functional for); for which we implement a two-step form. Since this problem assumes homoskedasticity, we can proceed with the first option.\n(ii). Derive the variance of your estimator.\nIf homoskedastic:\n\\[\n\\hat{\\Omega} = \\frac{1}{N}\\sum_{i=1}^N z_i'\n\\begin{bmatrix}\n  2 & -1 & 0 \\\\\n  -1 & 2 & -1 \\\\\n  0 & -1 & 2 \\\\\n\\end{bmatrix}\nz_i\n\\]\nNote that if \\(\\mu_{it}\\) were serially correlated, our model would fail here.\nUsually, we are heteroskedastic. So we must get an estimate of Var(zi’deltamu_i) in the first step:\ni.e. an estimate for \\(E[z_i'\\Delta \\mu_i \\Delta \\mu_i ' z_i]\\)\nstep 1: use residuals from _{gmm} using whatever weighting matrix we want (we can even compute it the same step as above, assuming homosked)\n\\[\n\\hat{\\Omega}_{\\text{optimal}} = \\frac{1}{N}\\sum z_i'\\Delta \\hat{\\mu}_i \\Delta \\hat{\\mu}_i ' z_i\n\\]\nstep 2: plug in this newly found ‘optimal’ weighting matrix back into our GMM equation, and we find the optimal variance to take the form:\n\\[\n\\begin{align}\n  \\widehat{Avar}(\\hat{\\beta}_{GMM}) = \\frac{1}{N}\\left(\n  \\frac{\\sum k_i' z_i}{N} \\hat{\\Omega}_{optimal}^{-1} \\frac{\\sum z_i' k_i}{N}\n  \\right)^{-1}\n\\end{align}\n\\]\n\nSuppose \\(z_{it} = y_{it-1}\\) and you know that \\(\\gamma = 1\\). How would you estimate the model? Provide the estimator and its variance matrix.\n\nOur model would then take the form:\n\\[y_{it} = \\alpha + x_{it}\\beta + y_{it-1} + f_i + u_{it} \\]\nWhich leads us to the standard FE model:\n\\[\\Delta y_{it} = \\alpha + x_{it}\\beta + f_i + u_{it}\\]\nWhich leads us to the FE estimate:\n\\[\\hat{\\beta}_{FE} = \\left(\\sum_{i=1}^N \\tilde{X}_i'\\tilde{X}_i \\right)^{-1} \\sum_{i=1}^N \\tilde{X}_i'\\Delta \\tilde{y}_i\\]\nWhich has the following variance form:\n\\[\\widehat{Avar}(\\hat{\\beta}_{FE}) = \\sigma^2 \\left(\\sum_{i=1}^N \\tilde{X}_i'\\tilde{X}_i \\right)^{-1}\\]\n\n\nQuestion 3: Empirical\n\nlibrary(Statamarkdown)\n\n\nuse \"soep_lebensz_en.dta\", clear\n\n// question 1 //\ngen has_kids = (no_kids &gt; 0 & no_kids != .) // has_kids dummy\nlabel var has_kids \"kids dummy\"\n\nsort id year\nby id: gen obs_no = _n \nkeep if obs_no &lt;= 2  // only keep the first two obs for an individal\n\nby id: gen total_obs = _N\nkeep if total_obs == 2 // only keep obs with exactly two obs\n\nby id: gen year_gap = (year-year[_n-1])\nby id: egen total_gap = max(year_gap)\nkeep if total_gap == 1 // only keep obs with exactly one year differences\n\nsort id year\nxtset id year\n\n(SOEPINFO: Magic at Work! http://panel.gsoep.de/soepinfo/ (randomized with soep\n&gt; no)\n\n\n\n\n\n(6,505 observations deleted)\n\n\n(683 observations deleted)\n\n(2,867 missing values generated)\n\n\n(170 observations deleted)\n\n\n\nPanel variable: id (weakly balanced)\n Time variable: year, 2000 to 2004\n         Delta: 1 unit\n\n\n\n// first-diff \nreg d.satisf_std d.has_kids d.health_std d.education, noconstant\nestimate store firstdiff1\n\n// fixed effects\nxtreg satisf_std has_kids health_std education, fe\nestimate store fixed1\n\nno variables defined\nr(111);\n\nr(111);\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Notes/13heckman/heckman.html",
    "href": "Notes/13heckman/heckman.html",
    "title": "Heckit Estimator",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Notes/11ordered_response/ordered_response.html",
    "href": "Notes/11ordered_response/ordered_response.html",
    "title": "Ordered Response Models",
    "section": "",
    "text": "So far, we have focused on unordered choice models, in which there was no natural ordering of alternatives or sequencing of decisions. We now look at the case where alternatives can be ordered, which then can be estimated with so-called Ordered Response Models.\n\n\n\nTo understand the ordered response model, we first need to define a Latent Variable Model:\nWe assume a latent (unobserved) outcome variable \\(y_i^*\\), which can be thought of as “Happiness” or “Utility”:\n\\[\ny_i^* = x_i'\\beta + u_i\n\\]\n\nNote: No intercept is included in this specification. This is because if a constant exists when we try to calculate \\(\\mathbb{P}[\\varepsilon_i \\leq \\alpha_m - x_i'\\beta]\\), we cannot separately identify the constant (intercept) from the thresholds.\nProperties:\n\nThe observed values \\(y_i \\in \\{0, 1, 2, \\dots, 10\\}\\) are not cardinal.\nMagnitude is meaningless (e.g., the difference between 1 and 2 is not necessarily the same as the difference between 3 and 4).\nLinearity assumptions do not hold for the outcome categories.\n\n\nThis explains why OLS fails in this case; OLS implicitly assumes equidistant thresholds; i.e., the distance between \\(y_i = 1\\) & \\(y_i = 2\\) is the exact same as the distance between \\(y_i = 2\\) & \\(y_i = 3\\). Applied to our latent variable model, the ‘width’ of a category is determined by the distance between thresholds \\((\\text{e.g. } \\alpha_2 - \\alpha_1)\\). Let’s say that \\(y_i^*\\) represents life satisfaction; the thresholds for life satisfaction \\(\\alpha_1\\) \\(\\dots\\) \\(\\alpha_m\\) likely assume a non-equal form resembling the following structure:\n\\[ \\mid \\dots \\dots \\dots. \\mid \\dots \\dots. \\mid \\dots .\\mid ...\\mid . \\mid  .\\mid ... \\mid \\dots. \\mid \\dots \\dots.\\mid \\dots \\dots \\dots. \\mid\\]\n\n\nThe observed variable \\(y_i\\) is determined by where the latent variable \\(y_i^*\\) falls relative to thresholds (cut-points) \\(\\alpha_m\\):\n\\[\ny_i =\n\\begin{cases}\n1 & \\text{if } \\alpha_0 &lt; y_i^* \\leq \\alpha_1 \\quad (\\text{Assumed } \\alpha_0 = -\\infty) \\\\\n2 & \\text{if } \\alpha_1 &lt; y_i^* \\leq \\alpha_2 \\\\\n\\vdots & \\vdots \\\\\nm & \\text{if } \\alpha_{m-1} &lt; y_i^* \\leq \\alpha_m \\quad (\\text{Assumed } \\alpha_m = +\\infty)\n\\end{cases}\n\\]\n\n\n\nDeriving the probabilities for each outcome category conditional on \\(x_i\\):\n\n\n\\[\n\\begin{aligned}\n\\mathbb{P}[y_i = 1 \\mid X_i] &= \\mathbb{P}[y_i^* \\leq \\alpha_1 \\mid X_i] \\\\\n&= \\mathbb{P}[x_i'\\beta + u_i \\leq \\alpha_1 \\mid X_i] \\\\\n&= \\mathbb{P}[u_i \\leq \\alpha_1 - x_i'\\beta \\mid X_i]\n\\end{aligned}\n\\]\n\nNow depends on the distribution assumed on \\(u_i\\).\nFor example, if \\(u_i \\sim \\mathcal{N}(0, 1)\\) (Standard Normal):\n\n\\[\n= F_x(\\alpha_1 - x_i'\\beta)= \\Phi(\\alpha_1 - x_i'\\beta)\n\\]\nWhich gives us the Ordered Probit Model\n\nNote: If \\(u_i \\sim \\mathcal{N}(0, \\sigma^2)\\), we divide by sigma to normalize:\n\n\\[\n\\Phi\\left[ \\frac{u_i}{\\sigma} \\leq \\frac{\\alpha_1}{\\sigma} - x_i'\\frac{\\beta}{\\sigma} \\mid X_i \\right]\n\\]\n\n\n\n\\[\n\\begin{aligned}\n\\mathbb{P}[y_i = 2 \\mid x_i] &= \\mathbb{P}[\\alpha_1 &lt; y_i^* \\leq \\alpha_2 \\mid X_i] \\\\\n&= \\mathbb{P}[y_i^* \\leq \\alpha_2 \\mid X_i] - \\mathbb{P}[y_i^* \\leq \\alpha_1 \\mid X_i] \\\\\n&= \\Phi(\\alpha_2 - x_i'\\beta) - \\Phi(\\alpha_1 - x_i'\\beta)\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n\\mathbb{P}[y_i = m \\mid X_i] &= \\mathbb{P}[y_i^* &gt; \\alpha_{m-1} \\mid X_i] \\\\\n&= 1 - \\Phi(\\alpha_{m-1} - x_i'\\beta)\n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe log-likelihood for an individual \\(i\\), denoted \\(\\ell_i(\\alpha, \\beta)\\), combines the probabilities using indicator functions \\(\\mathbf{1}[\\cdot]\\):\n\\[\n\\begin{aligned}\n\\ell_i(\\alpha, \\beta) &= \\mathbf{1}[y_i=1] \\log \\Phi(\\alpha_1 - x_i'\\beta) \\\\\n&+ \\mathbf{1}[y_i=2] \\log [\\Phi(\\alpha_2 - x_i'\\beta) - \\Phi(\\alpha_1 - x_i'\\beta)] \\\\\n&   \\dots \\\\\n& + \\mathbf{1}[y_i=m] \\log \\Phi(\\alpha_{m-1} - x_i'\\beta)\n\\end{aligned}\n\\]\n\\(\\Rightarrow\\) MLE yields estimates \\(\\hat{\\beta}, \\hat{\\alpha}\\).\n\n\nAn advantage of such models is that they are parsimonious: they only require \\(K + (m-1)\\) coefficient estimates \\((\\# \\beta + \\# \\alpha)\\). Compare this to multinomial logit, which requires: \\((K+1) \\cdot (m-1)\\) coefficient estimates.\nTo see this, consider the following example:\nImagine predicting travel mode choice based on Income.\n\nOrdered (Low, Med, High Quality): Higher income pushes you up the quality ladder uniformly. We need just one slope estimate for “Income” and cut-points for “Low/Med” and “Med/High”.\nMultinomial (Bus, Bike, Car): Higher income might make you more likely to drive a Car but less likely to Bike. The effect of income is fundamentally different for each outcome, so we need to estimate separate slope parameters for each alternative.\n\n\n\n\n\nWe are interested in the impact of a change in regressor \\(x_i\\) on the probability of choosing alternative \\(j\\):\n\\[\n\\frac{\\partial \\mathbb{P}[y_i=j \\mid x_i]}{\\partial x_i}\n\\]\nUsing the Chain Rule on the probability definitions (where \\(F' = f\\), the PDF):\n\\[\n= \\left[ F'(\\alpha_{j-1} - x_i'\\beta) - F'(\\alpha_j - x_i'\\beta) \\right] \\beta\n\\]\n\nWhere \\(F\\) corresponds to the CDF (\\(\\Phi\\) for Probit, \\(\\Lambda\\) for Logit).\nThe term in brackets determines the sign of the effect (can be \\(&gt;0\\) or \\(&lt;0\\)).\nHowever, the sign is ambiguous: we cannot use the sign of \\(\\beta\\) to interpret the sign of the marginal effect."
  },
  {
    "objectID": "Notes/11ordered_response/ordered_response.html#motivation",
    "href": "Notes/11ordered_response/ordered_response.html#motivation",
    "title": "Ordered Response Models",
    "section": "",
    "text": "So far, we have focused on unordered choice models, in which there was no natural ordering of alternatives or sequencing of decisions. We now look at the case where alternatives can be ordered, which then can be estimated with so-called Ordered Response Models."
  },
  {
    "objectID": "Notes/11ordered_response/ordered_response.html#ordered-response-model",
    "href": "Notes/11ordered_response/ordered_response.html#ordered-response-model",
    "title": "Ordered Response Models",
    "section": "",
    "text": "To understand the ordered response model, we first need to define a Latent Variable Model:\nWe assume a latent (unobserved) outcome variable \\(y_i^*\\), which can be thought of as “Happiness” or “Utility”:\n\\[\ny_i^* = x_i'\\beta + u_i\n\\]\n\nNote: No intercept is included in this specification. This is because if a constant exists when we try to calculate \\(\\mathbb{P}[\\varepsilon_i \\leq \\alpha_m - x_i'\\beta]\\), we cannot separately identify the constant (intercept) from the thresholds.\nProperties:\n\nThe observed values \\(y_i \\in \\{0, 1, 2, \\dots, 10\\}\\) are not cardinal.\nMagnitude is meaningless (e.g., the difference between 1 and 2 is not necessarily the same as the difference between 3 and 4).\nLinearity assumptions do not hold for the outcome categories.\n\n\nThis explains why OLS fails in this case; OLS implicitly assumes equidistant thresholds; i.e., the distance between \\(y_i = 1\\) & \\(y_i = 2\\) is the exact same as the distance between \\(y_i = 2\\) & \\(y_i = 3\\). Applied to our latent variable model, the ‘width’ of a category is determined by the distance between thresholds \\((\\text{e.g. } \\alpha_2 - \\alpha_1)\\). Let’s say that \\(y_i^*\\) represents life satisfaction; the thresholds for life satisfaction \\(\\alpha_1\\) \\(\\dots\\) \\(\\alpha_m\\) likely assume a non-equal form resembling the following structure:\n\\[ \\mid \\dots \\dots \\dots. \\mid \\dots \\dots. \\mid \\dots .\\mid ...\\mid . \\mid  .\\mid ... \\mid \\dots. \\mid \\dots \\dots.\\mid \\dots \\dots \\dots. \\mid\\]\n\n\nThe observed variable \\(y_i\\) is determined by where the latent variable \\(y_i^*\\) falls relative to thresholds (cut-points) \\(\\alpha_m\\):\n\\[\ny_i =\n\\begin{cases}\n1 & \\text{if } \\alpha_0 &lt; y_i^* \\leq \\alpha_1 \\quad (\\text{Assumed } \\alpha_0 = -\\infty) \\\\\n2 & \\text{if } \\alpha_1 &lt; y_i^* \\leq \\alpha_2 \\\\\n\\vdots & \\vdots \\\\\nm & \\text{if } \\alpha_{m-1} &lt; y_i^* \\leq \\alpha_m \\quad (\\text{Assumed } \\alpha_m = +\\infty)\n\\end{cases}\n\\]\n\n\n\nDeriving the probabilities for each outcome category conditional on \\(x_i\\):\n\n\n\\[\n\\begin{aligned}\n\\mathbb{P}[y_i = 1 \\mid X_i] &= \\mathbb{P}[y_i^* \\leq \\alpha_1 \\mid X_i] \\\\\n&= \\mathbb{P}[x_i'\\beta + u_i \\leq \\alpha_1 \\mid X_i] \\\\\n&= \\mathbb{P}[u_i \\leq \\alpha_1 - x_i'\\beta \\mid X_i]\n\\end{aligned}\n\\]\n\nNow depends on the distribution assumed on \\(u_i\\).\nFor example, if \\(u_i \\sim \\mathcal{N}(0, 1)\\) (Standard Normal):\n\n\\[\n= F_x(\\alpha_1 - x_i'\\beta)= \\Phi(\\alpha_1 - x_i'\\beta)\n\\]\nWhich gives us the Ordered Probit Model\n\nNote: If \\(u_i \\sim \\mathcal{N}(0, \\sigma^2)\\), we divide by sigma to normalize:\n\n\\[\n\\Phi\\left[ \\frac{u_i}{\\sigma} \\leq \\frac{\\alpha_1}{\\sigma} - x_i'\\frac{\\beta}{\\sigma} \\mid X_i \\right]\n\\]\n\n\n\n\\[\n\\begin{aligned}\n\\mathbb{P}[y_i = 2 \\mid x_i] &= \\mathbb{P}[\\alpha_1 &lt; y_i^* \\leq \\alpha_2 \\mid X_i] \\\\\n&= \\mathbb{P}[y_i^* \\leq \\alpha_2 \\mid X_i] - \\mathbb{P}[y_i^* \\leq \\alpha_1 \\mid X_i] \\\\\n&= \\Phi(\\alpha_2 - x_i'\\beta) - \\Phi(\\alpha_1 - x_i'\\beta)\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n\\mathbb{P}[y_i = m \\mid X_i] &= \\mathbb{P}[y_i^* &gt; \\alpha_{m-1} \\mid X_i] \\\\\n&= 1 - \\Phi(\\alpha_{m-1} - x_i'\\beta)\n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe log-likelihood for an individual \\(i\\), denoted \\(\\ell_i(\\alpha, \\beta)\\), combines the probabilities using indicator functions \\(\\mathbf{1}[\\cdot]\\):\n\\[\n\\begin{aligned}\n\\ell_i(\\alpha, \\beta) &= \\mathbf{1}[y_i=1] \\log \\Phi(\\alpha_1 - x_i'\\beta) \\\\\n&+ \\mathbf{1}[y_i=2] \\log [\\Phi(\\alpha_2 - x_i'\\beta) - \\Phi(\\alpha_1 - x_i'\\beta)] \\\\\n&   \\dots \\\\\n& + \\mathbf{1}[y_i=m] \\log \\Phi(\\alpha_{m-1} - x_i'\\beta)\n\\end{aligned}\n\\]\n\\(\\Rightarrow\\) MLE yields estimates \\(\\hat{\\beta}, \\hat{\\alpha}\\).\n\n\nAn advantage of such models is that they are parsimonious: they only require \\(K + (m-1)\\) coefficient estimates \\((\\# \\beta + \\# \\alpha)\\). Compare this to multinomial logit, which requires: \\((K+1) \\cdot (m-1)\\) coefficient estimates.\nTo see this, consider the following example:\nImagine predicting travel mode choice based on Income.\n\nOrdered (Low, Med, High Quality): Higher income pushes you up the quality ladder uniformly. We need just one slope estimate for “Income” and cut-points for “Low/Med” and “Med/High”.\nMultinomial (Bus, Bike, Car): Higher income might make you more likely to drive a Car but less likely to Bike. The effect of income is fundamentally different for each outcome, so we need to estimate separate slope parameters for each alternative.\n\n\n\n\n\nWe are interested in the impact of a change in regressor \\(x_i\\) on the probability of choosing alternative \\(j\\):\n\\[\n\\frac{\\partial \\mathbb{P}[y_i=j \\mid x_i]}{\\partial x_i}\n\\]\nUsing the Chain Rule on the probability definitions (where \\(F' = f\\), the PDF):\n\\[\n= \\left[ F'(\\alpha_{j-1} - x_i'\\beta) - F'(\\alpha_j - x_i'\\beta) \\right] \\beta\n\\]\n\nWhere \\(F\\) corresponds to the CDF (\\(\\Phi\\) for Probit, \\(\\Lambda\\) for Logit).\nThe term in brackets determines the sign of the effect (can be \\(&gt;0\\) or \\(&lt;0\\)).\nHowever, the sign is ambiguous: we cannot use the sign of \\(\\beta\\) to interpret the sign of the marginal effect."
  },
  {
    "objectID": "Notes/09multinomial/multinomial.html",
    "href": "Notes/09multinomial/multinomial.html",
    "title": "Multinomial Models",
    "section": "",
    "text": "A natural question that arises regarding our log-likelihood models for discrete outcomes is that of model specification. Specifically, how do we select a form for \\(F_j (x_{i},\\beta)\\)?\nSuppose we have \\(j=1,2,...,m\\) alternatives. We can then define the following:\n\\[U_j = V_j + \\varepsilon_j \\tag{1}\\]\nWhere:\n\n\\(U_j\\) : represents the utility of choosing alternative \\(j\\)\n\\(V_j\\) : the deterministic component (comprised of all the possible regressors) for alternative \\(j\\)\n\\(\\varepsilon_j\\) : random error term – included since all individuals have the alternatives but possibly different utilities for a choice – allows for some randomness\n\nWhen does an individual prefer alternative \\(j\\) to alternative \\(k\\)? Simply when they get more utility from \\(j\\) than they do \\(k\\). More formally:\n\\[\n\\begin{aligned}\n\\mathbb{P}[y_i = j \\mid X_i] &= \\mathbb{P}[U_j \\ge U_k, \\forall_{j \\neq k}] \\\\\n&= \\mathbb{P}[U_k - U_j \\leq 0, \\forall_{j \\neq k}] \\\\\n&= \\mathbb{P}[\\varepsilon_k - \\varepsilon_j \\leq V_k - V_j, \\forall_{j \\neq k}] \\\\\n&= \\mathbb{P}[\\widetilde{\\varepsilon}_{kj} \\leq -\\widetilde{V}_{kj} , \\forall_{j \\neq k}]\n\\end{aligned} \\tag{2}\n\\]\nNote that in the last step we simply rewrite the difference between alternative \\(k\\) and alternative \\(j\\) as \\(\\tilde{\\varepsilon}_{kj}\\).\nLets explore this in a 3-choice model:\n\\[\n\\begin{aligned}\n\\mathbb{P}[y_i=1 \\mid X_i] = \\mathbb{P}[\\widetilde{\\varepsilon}_{31} \\leq -\\widetilde{V}_{31}, \\widetilde{\\varepsilon}_{21} \\leq -\\widetilde{V}_{21} \\mid X_i]\n\\end{aligned} \\tag{3}\n\\]\nThis is telling us the same thing as (2): the probability that individual \\(i\\) chooses alternative \\(1\\) (given a set of alternatives) is equal to the probability that individual \\(i\\) prefers alternative \\(1\\) to both alternative \\(3\\) as well as alternative \\(2\\).\nWe can write this as a joint probability density function of the error terms:\n\\[\n\\begin{aligned}\n= \\int_{-\\infty}^{-\\tilde{V}_{31}} \\int_{-\\infty}^{-\\tilde{V}_{21}} f\\left(\\tilde{\\varepsilon}_{31}, \\tilde{\\varepsilon}_{21} \\right) d \\tilde{\\varepsilon}_{21} d \\tilde{\\varepsilon}_{31}\n\\end{aligned} \\tag{4}\n\\]\nAlthough this is the correct theoretical specification, an issue arises when we try to compute this for more alternatives. Imagine we have a set of 20 alternatives; we would have a 19-degree integration which would be computationally impossible to evaluate in any finite time period. Thus, we need to simplify this integration somehow.\n\n\n\nTo simplify the model, a key assumption we need to make is on the distribution of the error terms; specifically we need to assume that the errors follow a Type 1 Extreme Value Distribution (Gumbel Distribution):\n\\[f\\left(\\varepsilon_j \\right) \\sim Gumbel\\left( \\mu, \\beta \\right)\\]\nWhich indirectly makes assumptions on the differenced errors (from our 3-alternative example above):\n\\[f\\left(\\tilde{\\varepsilon}_{21} ,  \\tilde{\\varepsilon}_{31} \\right) \\sim Gumbel\\left( \\mu, \\beta \\right)\\]\nWhere the Gumbel Distribution’s PDF has the following form:\n\\[ f(\\varepsilon_j) = e^{-\\varepsilon_j} \\cdot e^{-e^{-\\varepsilon_j}} \\tag{5}\\]\n\n\n\nFigure 1: PDF of a Gumbel Distributed Random Variable\n\n\n** Note: multinomial probit assumes a normal (Gaussian) distributional form of the error terms; we will not cover that case here.\nFurther, we also assume these errors are IID: importantly, this assumption is placed upon the alternatives, rather than the individuals (although that is always assumed in the background). This means that our error terms are uncorrelated across alternatives (\\(j = 1, 2, ... , m\\)); every alternative is drawn at random with equal probability.\nNote:\nThis is a strong assumption that will be relaxed later on. For now, imagine the case where our set of alternatives consists of different cities an individual could to move to, say:\n\\[\\{\\text{Alternatives}\\}_m = \\{\\text{Barcelona, Madrid, Beijing, Moscow}\\}. \\]\nIntuitively, an individual’s utility for Barcelona and Madrid are likely highly correlated as they share similar characteristics (location, weather, language, etc), compared to other cities in our set, such as Beijing or Moscow. The IID assumption ignores this correlation, treating a preference for Barcelona as completely independent of a preference for Madrid.\nThe reason we choose the Gumbel distribution for our errors is that, after substituting the Gumbel PDF from (5) into our joint PDF function from (4), our conditional probability function simplifies to:\n\\[ \\mathbb{P}[y_i = j \\mid X_i] = \\frac{e^{V_j}}{e^{V_1} + e^{V_2} + ... + e^{V_m}} \\tag{6}\\]\nThis result is much more computationally friendly, even when as th choice set increases to 20 or 30 alternatives, the Gumbel distribution allows for a closed-form solution (6).\nFurther, although the Gumbel distribution is asymmetrical, it is the standard choice here because it is an Extreme Value distribution. It is designed to model the maximum of a series of random variables, making it ideal for our utility framework where we assume the individual selects the alternative that yields the maximum utility.\nIn the real world, this distributional form is useful in predicting the chance that an extreme event will occur (earthquake prediction simulations, measuring financial losses, insurance risks, etc).\n\n\n\nWhat remains is a specification of the deterministic component of the utility function. There exists two main specification methods (and a mixture between them):\n\\[\nV_j = \\begin{cases}\n  X_{ij}'\\beta &\\text{\"Conditional Logit\"} \\\\\n   \\\\\n  X_i'\\beta_j &\\text{\"Multinomial Logit\"} \\\\\n   \\\\\n  X_{ij}'\\beta + \\omega_i'\\alpha_j &\\text{\"Mixed Logit\"}\n\\end{cases} \\tag{7}\n\\]\n\n\nWe specify the deterministic component of utility of alternative \\(j\\) with alternative-varying regressors, i.e. \n\\[V_j = X_{ij}'\\beta \\tag{8}\\]\nThese regressors can vary across individuals and alternatives. Using our previous choice set of cities, we can think of these regressors as being characteristics of the cities themselves impacting the overall utility: weather, location, population, etc; varying across individuals.\nThese are the characteristics the regressors share but can vary in their utility for each individual. I.e. I might value the warm weather of Barcelona more than the cold of Moscow, but some other individual might value the inverse case.\nPlugging in this specification for \\(V_j\\) into equation (1), we get the following form for the likelihood function:\n\\[P_{ij} = \\mathbb{P}[y_i = j \\mid X_i] = \\frac{e^{x_{ij}'\\beta}}{\\displaystyle\\sum_{k=1}^me^{x_{ik}'\\beta}} \\tag{9}\\]\n\n\n\nIn this case, the specified regressors contained in the deterministic component of utility are individual-specific, but not alternative-specific. We define these as alternative-invariant regressors:\n\\[V_j = X_i'\\beta_j \\tag{10}\\]\nUsing our same cities choice set, we can think of this being individual-level characteristics affecting the utility assigned to a specific city; my current lifestyle/age will impact the utility I receive moving to a city like Barcelona versus a city like Moscow; however, characteristics of the city itself (its party scene, age of population, etc) has no effect on my utility assigned to moving to that city. (personal note: I find this hard to believe; I would expect that the utility you get from aspects like your age are tied in directly to the characteristics of the cities themselves. maybe another example would make this specifiation choice more clear, but it seems unrealistic for our correct model specification to be mlogit.)\nPlugging in this specification for \\(V_j\\) into equation (1), we get the following form for the likelihood function:\n\\[P_{ij} = \\mathbb{P}[y_i = j \\mid X_i] = \\frac{e^{x_i'\\beta_j}}{\\displaystyle\\sum_{k=1}^m e^{x_i'\\beta_k}} \\tag{11}\\]\n\n\n\n\nLet’s look at another example. Suppose that we are fishing in Barcelona, and we have the following set of alternatives for fishing locations:\n\\[\\{\\text{Fishing Choice Alternatives}\\}_{m=4} = \\{\\text{Beach}_1, \\text{Pier}_2, \\text{Private Boat}_3, \\text{Charter Boat}_4\\}\\]\n\n\n\\[U_{ij} = X_{ij}'\\beta + \\varepsilon_{ij} \\tag{12}\\]\nSuppose we define the vector of regressors in \\(X_{ij}\\) as the following:\n\\[\nX_{ij} = \\begin{bmatrix}\n  \\text{Catch Rate} \\\\\n  \\text{Price}\n\\end{bmatrix}\n\\]\nNote that these regressors are alternative-variant:\n\nCatch Rate depends on the choice of beach, boat, pier, etc;\nsame goes for the Price of fishing at a given location.\n\n\n\nThe marginal change in the probability of fishing at the beach is the marginal effect given by beta; in other words, how the probability of individual \\(i\\) choosing a specific location (e.g., \\(\\Delta P_{i,\\text{beach}}\\)) changes when a characteristic of that same location (e.g., \\(\\Delta \\text{Catch Rate}_{i, \\text{Beach}}\\)) changes.\n\\[\n\\begin{align}\n\\frac{\\partial P_{i1}}{\\partial X_{i1}}\n&= \\frac{\\partial \\mathbb{P}[y_i = \\text{Beach} \\mid \\{\\text{Catch Rate, Price} \\}_i]}{\\partial X_{i, \\text{Beach}}} \\\\\n&= \\left( \\underbrace{\\frac{e^{x_{i1}'\\beta}}{\\sum_{k=1}^m e^{x_{ik}'\\beta}}}_{P_{i1}} - \\underbrace{\\left( \\frac{e^{x_{i1}'\\beta}}{\\sum_{k=1}^m e^{x_{ik}'\\beta}} \\right)^2}_{(P_{i1})^2} \\right) \\beta \\\\\n&= P_{i1}(1-P_{i1})\\beta \\tag{14}\n\\end{align}\n\\]\nWhat does this tell us?\n\nBeta tells us the sign of the marginal effect: since \\(P_{ij} \\in [0, 1]\\), it must be that if beta is positive, so too is the marginal effect.\nThe marginal effect coffecient value is not directly interpretable – we cannot separate \\(P_{ij}\\) and \\(1 - P_{ij}\\) here.\n\n\n\n\nWhat if we look across alternatives? I.e., how does a change in the characteristics of the Beach (\\(\\Delta X_{i,\\text{Beach}}\\)) affect the probability of individual \\(i\\) choosing to fish from the Pier (\\(\\Delta P_{i, \\text{Pier}}\\))?\nEquivalently, this is expressed as:\n\\[\n\\begin{align}\n\\frac\n{\\partial \\mathbb{P}[y_i = Pier \\mid \\{\\text{Catch Rate, Price} \\}_i]}\n{\\partial X_{i, Beach}}\n&=\n\\frac\n{\\partial P_{i2}}\n{\\partial X_{i1}} \\\\\n&=\n\\frac\n{0 - e^{x_{i2}'\\beta}e^{x_{i1}'\\beta} \\beta}\n{\\left(\\sum_{k=1}^me^{x_{ik}'\\beta}\\right)^2} \\\\\n&=\n(-P_{i2})P_{i1}\\beta \\tag{15}\n\\end{align}\n\\]\nObservations:\n\nBeta and the Marginal Effect have opposing signs: if \\(\\beta \\gt 0\\), then \\(ME(\\beta) \\lt 0\\)\nAgain, cannot interpret the magnitude of beta.\n\nIn summary: we can write the two types of marginal effects as the following for a conditional logit model:\n\\[\n\\frac{\\partial P_{ij}}{\\partial X_{ik}} =\n\\begin{cases}\n  P_{ij}(1-P_{ij}) & j = k \\\\\n  P_{ij}(-P_{ik}) & j \\neq k\n\\end{cases} \\tag{16}\n\\]\nNote that the direction is unambiguous for both cases.\n\n\n\n\nWhat happens when applied to a multinomial logit model? Is the marginal effect the same?\nThe marginal effect is written as:\n\\[\\frac{\\partial P_{ij}}{\\partial X_i} = P_{ij}(\\beta_j - \\bar{\\beta}_i), \\\\ \\text{where } \\bar{\\beta}_i \\equiv \\sum_{k=1}^m P_{ik}\\beta_k \\tag{17}\\]\nWhere \\(\\bar{\\beta}_i\\) is a “weighted average” of \\(\\beta\\)’s (coefficients), weighted by the probability of choosing a particular alternative \\(k\\).\nThe issue here is that finding the sign of \\(\\beta_j\\) is not informative of the ME – it depends on the value of \\(\\bar{\\beta}_i\\); i.e., the direction is ambiguous.\n\n\n\nAnother way to interpret our coefficients is looking at the relative risk ratio (rrr), by taking the ratio of the probabilities of two distinct alternatives\n\n\n\\[\n\\begin{align}\n\\frac\n{P_{ij}}\n{P_{ik}} &=\n\\frac\n{\\mathbb{P}[y_i = j \\mid X_i]}\n{\\mathbb{P}[y_i = k \\mid X_i]} \\\\\n&=\n\\frac\n{e^{x_{ij}'\\beta}}\n{e^{x_{ik}'\\beta}} && \\text{by conditional logit def} \\\\\n&=\ne^{(x_{ij}-x_{ik})'\\beta} && \\text{property of exponents} \\\\\nlog\n\\frac\n{P_{ij}}\n{P_{ik}} &=\n(X_{ij}-X_{ik})'\\beta && \\text{log both sides} \\tag{18}\n\\end{align}\n\\]\nThus, we can interpret \\(\\beta\\) as the change in the log-odds of choosing alternative \\(j\\) over alternative \\(k\\) for every one-unit increase in the difference between their characteristics (\\(X_{ij}\\)-\\(X_{ik}\\))\nIn our fishing example, suppose we get output of \\(\\beta_{price} = -0.05\\) for (\\(X_{i,Beach} - X_{i,Pier}\\)) – we would interpret this as: for a $1 increase in the price of the Beach (relative to the Pier), the odds of choosing the Beach over the Pier drop by 5%.\n\n\n\nFor mlogit, its important to standardize a beta, essentially to set one alternative as our “base case” with which we can relatively compare the other alternatives. For example, suppose we set \\(\\beta_k = \\beta_1 = 0\\); then, our relative risk ratio (also know as odds-ratio) is derived as the following:\n\\[\n\\begin{align}\n\\frac\n{P_{ij}}\n{P_{ik}} &=\n\\frac\n{\\mathbb{P}[y_i = j \\mid X_i]}\n{\\mathbb{P}[y_i = k \\mid X_i]} \\\\\n&=\n\\frac\n{e^{x_i'\\beta_j}}\n{e^{x_i'\\beta_k}} && \\text{by conditional logit def} \\\\\n&=\ne^{x_i'(\\beta_j-\\beta_k)} && \\text{property of exponents} \\\\\n&=\n\\frac\n{\\mathbb{P}[y_i = j \\mid X_i]}\n{\\mathbb{P}[y_i = 1 \\mid X_i]} && \\text{plug in } \\beta_k  = \\beta_1 \\\\\n&=\ne^{x_i'\\beta_j}\n&& \\text{normalize: } \\beta_1 = 0\\\\\nlog\n\\frac\n{P_{ij}}\n{P_{i1}} &=\n(X_i)'\\beta_j && \\text{log both sides} \\tag{19}\n\\end{align}\n\\]\nHere we can interpret \\(\\beta\\) as the change in the log-odds of choosing alternative \\(j\\) relative to the base case (alternative \\(k\\)) for a one-unit increase in individual-specific regressors (e.g., Income). Note that unlike the clogit model, where we look at the difference in characteristics between choices, here the characteristic is fixed for the individual, and the coefficient \\(\\beta_j\\) represents how that trait shifts the individual’s preference toward alternative \\(j\\) specifically.\n\n\n\n\n\nA seemingly powerful aspect of these models is the fact that the probabilities only depend the two specific alternatives we are comparing at a given time, stemming from out IID assumption early on. However, we can see an example as to why this might fail in reality.\nSuppose we want to model traffic in a city, and there exists only two forms of transportation, a car and a red bus, s.t.:\n\\[\\frac{\\mathbb{P}[\\text{car}]}{\\mathbb{P}[\\text{red bus}]} = \\frac{0.5}{0.5}\\]\n\nNote that this is consistent since \\(\\displaystyle\\sum_{i=1}^N\\mathbb{P}_i = 1\\)\n\nNow imagine we introduce a new form of transportation (i.e. a third alternative) in the city: a blue bus. Intuitively, we expect to see:\n\\[\n\\begin{align}\n\\mathbb{P}[\\text{blue bus}] = \\mathbb{P}[\\text{red bus}] = 25\\% \\\\\n\\mathbb{P}[\\text{car}] = 50\\%\n\\end{align}\n\\]\nYet for our multinomal models, we instead derive from the IID (random sampling) assumption that our new choice distribution is:\n\\[\\mathbb{P}[\\text{blue bus}] = \\mathbb{P}[\\text{red bus}] = \\mathbb{P}[\\text{car}] = 33.33\\%\\] Another way to say the same thing is that an improved product gains share from all other products in proportion to their original shares; and when a product loses share, it loses to others in proportion to their shares.\nAs it stands, our current models cannot account for this phenomena; hence, we need to relax our IID assumption and try to construct a different variant of our models: Nested Logit."
  },
  {
    "objectID": "Notes/09multinomial/multinomial.html#specification-p_ijy_ij",
    "href": "Notes/09multinomial/multinomial.html#specification-p_ijy_ij",
    "title": "Multinomial Models",
    "section": "",
    "text": "A natural question that arises regarding our log-likelihood models for discrete outcomes is that of model specification. Specifically, how do we select a form for \\(F_j (x_{i},\\beta)\\)?\nSuppose we have \\(j=1,2,...,m\\) alternatives. We can then define the following:\n\\[U_j = V_j + \\varepsilon_j \\tag{1}\\]\nWhere:\n\n\\(U_j\\) : represents the utility of choosing alternative \\(j\\)\n\\(V_j\\) : the deterministic component (comprised of all the possible regressors) for alternative \\(j\\)\n\\(\\varepsilon_j\\) : random error term – included since all individuals have the alternatives but possibly different utilities for a choice – allows for some randomness\n\nWhen does an individual prefer alternative \\(j\\) to alternative \\(k\\)? Simply when they get more utility from \\(j\\) than they do \\(k\\). More formally:\n\\[\n\\begin{aligned}\n\\mathbb{P}[y_i = j \\mid X_i] &= \\mathbb{P}[U_j \\ge U_k, \\forall_{j \\neq k}] \\\\\n&= \\mathbb{P}[U_k - U_j \\leq 0, \\forall_{j \\neq k}] \\\\\n&= \\mathbb{P}[\\varepsilon_k - \\varepsilon_j \\leq V_k - V_j, \\forall_{j \\neq k}] \\\\\n&= \\mathbb{P}[\\widetilde{\\varepsilon}_{kj} \\leq -\\widetilde{V}_{kj} , \\forall_{j \\neq k}]\n\\end{aligned} \\tag{2}\n\\]\nNote that in the last step we simply rewrite the difference between alternative \\(k\\) and alternative \\(j\\) as \\(\\tilde{\\varepsilon}_{kj}\\).\nLets explore this in a 3-choice model:\n\\[\n\\begin{aligned}\n\\mathbb{P}[y_i=1 \\mid X_i] = \\mathbb{P}[\\widetilde{\\varepsilon}_{31} \\leq -\\widetilde{V}_{31}, \\widetilde{\\varepsilon}_{21} \\leq -\\widetilde{V}_{21} \\mid X_i]\n\\end{aligned} \\tag{3}\n\\]\nThis is telling us the same thing as (2): the probability that individual \\(i\\) chooses alternative \\(1\\) (given a set of alternatives) is equal to the probability that individual \\(i\\) prefers alternative \\(1\\) to both alternative \\(3\\) as well as alternative \\(2\\).\nWe can write this as a joint probability density function of the error terms:\n\\[\n\\begin{aligned}\n= \\int_{-\\infty}^{-\\tilde{V}_{31}} \\int_{-\\infty}^{-\\tilde{V}_{21}} f\\left(\\tilde{\\varepsilon}_{31}, \\tilde{\\varepsilon}_{21} \\right) d \\tilde{\\varepsilon}_{21} d \\tilde{\\varepsilon}_{31}\n\\end{aligned} \\tag{4}\n\\]\nAlthough this is the correct theoretical specification, an issue arises when we try to compute this for more alternatives. Imagine we have a set of 20 alternatives; we would have a 19-degree integration which would be computationally impossible to evaluate in any finite time period. Thus, we need to simplify this integration somehow."
  },
  {
    "objectID": "Notes/09multinomial/multinomial.html#distribution-assumption-of-the-errors",
    "href": "Notes/09multinomial/multinomial.html#distribution-assumption-of-the-errors",
    "title": "Multinomial Models",
    "section": "",
    "text": "To simplify the model, a key assumption we need to make is on the distribution of the error terms; specifically we need to assume that the errors follow a Type 1 Extreme Value Distribution (Gumbel Distribution):\n\\[f\\left(\\varepsilon_j \\right) \\sim Gumbel\\left( \\mu, \\beta \\right)\\]\nWhich indirectly makes assumptions on the differenced errors (from our 3-alternative example above):\n\\[f\\left(\\tilde{\\varepsilon}_{21} ,  \\tilde{\\varepsilon}_{31} \\right) \\sim Gumbel\\left( \\mu, \\beta \\right)\\]\nWhere the Gumbel Distribution’s PDF has the following form:\n\\[ f(\\varepsilon_j) = e^{-\\varepsilon_j} \\cdot e^{-e^{-\\varepsilon_j}} \\tag{5}\\]\n\n\n\nFigure 1: PDF of a Gumbel Distributed Random Variable\n\n\n** Note: multinomial probit assumes a normal (Gaussian) distributional form of the error terms; we will not cover that case here.\nFurther, we also assume these errors are IID: importantly, this assumption is placed upon the alternatives, rather than the individuals (although that is always assumed in the background). This means that our error terms are uncorrelated across alternatives (\\(j = 1, 2, ... , m\\)); every alternative is drawn at random with equal probability.\nNote:\nThis is a strong assumption that will be relaxed later on. For now, imagine the case where our set of alternatives consists of different cities an individual could to move to, say:\n\\[\\{\\text{Alternatives}\\}_m = \\{\\text{Barcelona, Madrid, Beijing, Moscow}\\}. \\]\nIntuitively, an individual’s utility for Barcelona and Madrid are likely highly correlated as they share similar characteristics (location, weather, language, etc), compared to other cities in our set, such as Beijing or Moscow. The IID assumption ignores this correlation, treating a preference for Barcelona as completely independent of a preference for Madrid.\nThe reason we choose the Gumbel distribution for our errors is that, after substituting the Gumbel PDF from (5) into our joint PDF function from (4), our conditional probability function simplifies to:\n\\[ \\mathbb{P}[y_i = j \\mid X_i] = \\frac{e^{V_j}}{e^{V_1} + e^{V_2} + ... + e^{V_m}} \\tag{6}\\]\nThis result is much more computationally friendly, even when as th choice set increases to 20 or 30 alternatives, the Gumbel distribution allows for a closed-form solution (6).\nFurther, although the Gumbel distribution is asymmetrical, it is the standard choice here because it is an Extreme Value distribution. It is designed to model the maximum of a series of random variables, making it ideal for our utility framework where we assume the individual selects the alternative that yields the maximum utility.\nIn the real world, this distributional form is useful in predicting the chance that an extreme event will occur (earthquake prediction simulations, measuring financial losses, insurance risks, etc)."
  },
  {
    "objectID": "Notes/09multinomial/multinomial.html#specification-v_j",
    "href": "Notes/09multinomial/multinomial.html#specification-v_j",
    "title": "Multinomial Models",
    "section": "",
    "text": "What remains is a specification of the deterministic component of the utility function. There exists two main specification methods (and a mixture between them):\n\\[\nV_j = \\begin{cases}\n  X_{ij}'\\beta &\\text{\"Conditional Logit\"} \\\\\n   \\\\\n  X_i'\\beta_j &\\text{\"Multinomial Logit\"} \\\\\n   \\\\\n  X_{ij}'\\beta + \\omega_i'\\alpha_j &\\text{\"Mixed Logit\"}\n\\end{cases} \\tag{7}\n\\]\n\n\nWe specify the deterministic component of utility of alternative \\(j\\) with alternative-varying regressors, i.e. \n\\[V_j = X_{ij}'\\beta \\tag{8}\\]\nThese regressors can vary across individuals and alternatives. Using our previous choice set of cities, we can think of these regressors as being characteristics of the cities themselves impacting the overall utility: weather, location, population, etc; varying across individuals.\nThese are the characteristics the regressors share but can vary in their utility for each individual. I.e. I might value the warm weather of Barcelona more than the cold of Moscow, but some other individual might value the inverse case.\nPlugging in this specification for \\(V_j\\) into equation (1), we get the following form for the likelihood function:\n\\[P_{ij} = \\mathbb{P}[y_i = j \\mid X_i] = \\frac{e^{x_{ij}'\\beta}}{\\displaystyle\\sum_{k=1}^me^{x_{ik}'\\beta}} \\tag{9}\\]\n\n\n\nIn this case, the specified regressors contained in the deterministic component of utility are individual-specific, but not alternative-specific. We define these as alternative-invariant regressors:\n\\[V_j = X_i'\\beta_j \\tag{10}\\]\nUsing our same cities choice set, we can think of this being individual-level characteristics affecting the utility assigned to a specific city; my current lifestyle/age will impact the utility I receive moving to a city like Barcelona versus a city like Moscow; however, characteristics of the city itself (its party scene, age of population, etc) has no effect on my utility assigned to moving to that city. (personal note: I find this hard to believe; I would expect that the utility you get from aspects like your age are tied in directly to the characteristics of the cities themselves. maybe another example would make this specifiation choice more clear, but it seems unrealistic for our correct model specification to be mlogit.)\nPlugging in this specification for \\(V_j\\) into equation (1), we get the following form for the likelihood function:\n\\[P_{ij} = \\mathbb{P}[y_i = j \\mid X_i] = \\frac{e^{x_i'\\beta_j}}{\\displaystyle\\sum_{k=1}^m e^{x_i'\\beta_k}} \\tag{11}\\]"
  },
  {
    "objectID": "Notes/09multinomial/multinomial.html#marginal-effects",
    "href": "Notes/09multinomial/multinomial.html#marginal-effects",
    "title": "Multinomial Models",
    "section": "",
    "text": "Let’s look at another example. Suppose that we are fishing in Barcelona, and we have the following set of alternatives for fishing locations:\n\\[\\{\\text{Fishing Choice Alternatives}\\}_{m=4} = \\{\\text{Beach}_1, \\text{Pier}_2, \\text{Private Boat}_3, \\text{Charter Boat}_4\\}\\]\n\n\n\\[U_{ij} = X_{ij}'\\beta + \\varepsilon_{ij} \\tag{12}\\]\nSuppose we define the vector of regressors in \\(X_{ij}\\) as the following:\n\\[\nX_{ij} = \\begin{bmatrix}\n  \\text{Catch Rate} \\\\\n  \\text{Price}\n\\end{bmatrix}\n\\]\nNote that these regressors are alternative-variant:\n\nCatch Rate depends on the choice of beach, boat, pier, etc;\nsame goes for the Price of fishing at a given location.\n\n\n\nThe marginal change in the probability of fishing at the beach is the marginal effect given by beta; in other words, how the probability of individual \\(i\\) choosing a specific location (e.g., \\(\\Delta P_{i,\\text{beach}}\\)) changes when a characteristic of that same location (e.g., \\(\\Delta \\text{Catch Rate}_{i, \\text{Beach}}\\)) changes.\n\\[\n\\begin{align}\n\\frac{\\partial P_{i1}}{\\partial X_{i1}}\n&= \\frac{\\partial \\mathbb{P}[y_i = \\text{Beach} \\mid \\{\\text{Catch Rate, Price} \\}_i]}{\\partial X_{i, \\text{Beach}}} \\\\\n&= \\left( \\underbrace{\\frac{e^{x_{i1}'\\beta}}{\\sum_{k=1}^m e^{x_{ik}'\\beta}}}_{P_{i1}} - \\underbrace{\\left( \\frac{e^{x_{i1}'\\beta}}{\\sum_{k=1}^m e^{x_{ik}'\\beta}} \\right)^2}_{(P_{i1})^2} \\right) \\beta \\\\\n&= P_{i1}(1-P_{i1})\\beta \\tag{14}\n\\end{align}\n\\]\nWhat does this tell us?\n\nBeta tells us the sign of the marginal effect: since \\(P_{ij} \\in [0, 1]\\), it must be that if beta is positive, so too is the marginal effect.\nThe marginal effect coffecient value is not directly interpretable – we cannot separate \\(P_{ij}\\) and \\(1 - P_{ij}\\) here.\n\n\n\n\nWhat if we look across alternatives? I.e., how does a change in the characteristics of the Beach (\\(\\Delta X_{i,\\text{Beach}}\\)) affect the probability of individual \\(i\\) choosing to fish from the Pier (\\(\\Delta P_{i, \\text{Pier}}\\))?\nEquivalently, this is expressed as:\n\\[\n\\begin{align}\n\\frac\n{\\partial \\mathbb{P}[y_i = Pier \\mid \\{\\text{Catch Rate, Price} \\}_i]}\n{\\partial X_{i, Beach}}\n&=\n\\frac\n{\\partial P_{i2}}\n{\\partial X_{i1}} \\\\\n&=\n\\frac\n{0 - e^{x_{i2}'\\beta}e^{x_{i1}'\\beta} \\beta}\n{\\left(\\sum_{k=1}^me^{x_{ik}'\\beta}\\right)^2} \\\\\n&=\n(-P_{i2})P_{i1}\\beta \\tag{15}\n\\end{align}\n\\]\nObservations:\n\nBeta and the Marginal Effect have opposing signs: if \\(\\beta \\gt 0\\), then \\(ME(\\beta) \\lt 0\\)\nAgain, cannot interpret the magnitude of beta.\n\nIn summary: we can write the two types of marginal effects as the following for a conditional logit model:\n\\[\n\\frac{\\partial P_{ij}}{\\partial X_{ik}} =\n\\begin{cases}\n  P_{ij}(1-P_{ij}) & j = k \\\\\n  P_{ij}(-P_{ik}) & j \\neq k\n\\end{cases} \\tag{16}\n\\]\nNote that the direction is unambiguous for both cases.\n\n\n\n\nWhat happens when applied to a multinomial logit model? Is the marginal effect the same?\nThe marginal effect is written as:\n\\[\\frac{\\partial P_{ij}}{\\partial X_i} = P_{ij}(\\beta_j - \\bar{\\beta}_i), \\\\ \\text{where } \\bar{\\beta}_i \\equiv \\sum_{k=1}^m P_{ik}\\beta_k \\tag{17}\\]\nWhere \\(\\bar{\\beta}_i\\) is a “weighted average” of \\(\\beta\\)’s (coefficients), weighted by the probability of choosing a particular alternative \\(k\\).\nThe issue here is that finding the sign of \\(\\beta_j\\) is not informative of the ME – it depends on the value of \\(\\bar{\\beta}_i\\); i.e., the direction is ambiguous.\n\n\n\nAnother way to interpret our coefficients is looking at the relative risk ratio (rrr), by taking the ratio of the probabilities of two distinct alternatives\n\n\n\\[\n\\begin{align}\n\\frac\n{P_{ij}}\n{P_{ik}} &=\n\\frac\n{\\mathbb{P}[y_i = j \\mid X_i]}\n{\\mathbb{P}[y_i = k \\mid X_i]} \\\\\n&=\n\\frac\n{e^{x_{ij}'\\beta}}\n{e^{x_{ik}'\\beta}} && \\text{by conditional logit def} \\\\\n&=\ne^{(x_{ij}-x_{ik})'\\beta} && \\text{property of exponents} \\\\\nlog\n\\frac\n{P_{ij}}\n{P_{ik}} &=\n(X_{ij}-X_{ik})'\\beta && \\text{log both sides} \\tag{18}\n\\end{align}\n\\]\nThus, we can interpret \\(\\beta\\) as the change in the log-odds of choosing alternative \\(j\\) over alternative \\(k\\) for every one-unit increase in the difference between their characteristics (\\(X_{ij}\\)-\\(X_{ik}\\))\nIn our fishing example, suppose we get output of \\(\\beta_{price} = -0.05\\) for (\\(X_{i,Beach} - X_{i,Pier}\\)) – we would interpret this as: for a $1 increase in the price of the Beach (relative to the Pier), the odds of choosing the Beach over the Pier drop by 5%.\n\n\n\nFor mlogit, its important to standardize a beta, essentially to set one alternative as our “base case” with which we can relatively compare the other alternatives. For example, suppose we set \\(\\beta_k = \\beta_1 = 0\\); then, our relative risk ratio (also know as odds-ratio) is derived as the following:\n\\[\n\\begin{align}\n\\frac\n{P_{ij}}\n{P_{ik}} &=\n\\frac\n{\\mathbb{P}[y_i = j \\mid X_i]}\n{\\mathbb{P}[y_i = k \\mid X_i]} \\\\\n&=\n\\frac\n{e^{x_i'\\beta_j}}\n{e^{x_i'\\beta_k}} && \\text{by conditional logit def} \\\\\n&=\ne^{x_i'(\\beta_j-\\beta_k)} && \\text{property of exponents} \\\\\n&=\n\\frac\n{\\mathbb{P}[y_i = j \\mid X_i]}\n{\\mathbb{P}[y_i = 1 \\mid X_i]} && \\text{plug in } \\beta_k  = \\beta_1 \\\\\n&=\ne^{x_i'\\beta_j}\n&& \\text{normalize: } \\beta_1 = 0\\\\\nlog\n\\frac\n{P_{ij}}\n{P_{i1}} &=\n(X_i)'\\beta_j && \\text{log both sides} \\tag{19}\n\\end{align}\n\\]\nHere we can interpret \\(\\beta\\) as the change in the log-odds of choosing alternative \\(j\\) relative to the base case (alternative \\(k\\)) for a one-unit increase in individual-specific regressors (e.g., Income). Note that unlike the clogit model, where we look at the difference in characteristics between choices, here the characteristic is fixed for the individual, and the coefficient \\(\\beta_j\\) represents how that trait shifts the individual’s preference toward alternative \\(j\\) specifically."
  },
  {
    "objectID": "Notes/09multinomial/multinomial.html#shortcomings",
    "href": "Notes/09multinomial/multinomial.html#shortcomings",
    "title": "Multinomial Models",
    "section": "",
    "text": "A seemingly powerful aspect of these models is the fact that the probabilities only depend the two specific alternatives we are comparing at a given time, stemming from out IID assumption early on. However, we can see an example as to why this might fail in reality.\nSuppose we want to model traffic in a city, and there exists only two forms of transportation, a car and a red bus, s.t.:\n\\[\\frac{\\mathbb{P}[\\text{car}]}{\\mathbb{P}[\\text{red bus}]} = \\frac{0.5}{0.5}\\]\n\nNote that this is consistent since \\(\\displaystyle\\sum_{i=1}^N\\mathbb{P}_i = 1\\)\n\nNow imagine we introduce a new form of transportation (i.e. a third alternative) in the city: a blue bus. Intuitively, we expect to see:\n\\[\n\\begin{align}\n\\mathbb{P}[\\text{blue bus}] = \\mathbb{P}[\\text{red bus}] = 25\\% \\\\\n\\mathbb{P}[\\text{car}] = 50\\%\n\\end{align}\n\\]\nYet for our multinomal models, we instead derive from the IID (random sampling) assumption that our new choice distribution is:\n\\[\\mathbb{P}[\\text{blue bus}] = \\mathbb{P}[\\text{red bus}] = \\mathbb{P}[\\text{car}] = 33.33\\%\\] Another way to say the same thing is that an improved product gains share from all other products in proportion to their original shares; and when a product loses share, it loses to others in proportion to their shares.\nAs it stands, our current models cannot account for this phenomena; hence, we need to relax our IID assumption and try to construct a different variant of our models: Nested Logit."
  },
  {
    "objectID": "Notes/07mle/mle.html",
    "href": "Notes/07mle/mle.html",
    "title": "Maximum Likelihood Estimators",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Notes/06anderson/ahsiao.html",
    "href": "Notes/06anderson/ahsiao.html",
    "title": "Anderson-Hsiao Estimator",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Notes/04first_diff/fd.html",
    "href": "Notes/04first_diff/fd.html",
    "title": "First Differences",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Notes/02random_effects/re.html",
    "href": "Notes/02random_effects/re.html",
    "title": "Random Effects",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics: Panel Data",
    "section": "",
    "text": "flowchart TD\n    A(Panel Data) --&gt; B(Linear Models)\n    \n    B --&gt; D(Strict Exogeneity)\n    D --&gt; F(Random Effects)\n    D --&gt; G(Fixed Effects)\n    D --&gt; H(First Differences)\n    \n    B --&gt; E(Weak Exogeneity)\n    E --&gt; I(Anderson-Hsiao)\n    E --&gt; J(Arellano-Bond)\n    \n    \n    A --&gt; C(Maximum Likelihood)\n    \n    C --&gt; K(Discrete Choice)\n    K --&gt; M(Binary)\n    \n    K --&gt; N(Other)\n    N --&gt; O(Multinomial)\n    N --&gt; P(Nested Logit)\n    N --&gt; Q(Ordered Response)\n    \n    C --&gt; L(Tobit & Selection)\n    L --&gt; R(Heckman)\n    \n    \n    PS1([Problem Set 1])\n    PS2([Problem Set 2])\n    PS3([Problem Set 3])\n\n    %% Invisible link to nudge P near C/D without drawing a line\n    G ~~~ PS1\n    B ~~~ PS2\n    P ~~~ PS3\n\n    click D \"Notes/01pooled_ols/pols.html\" \"Go to Pooled OLS Notes\"\n    click F \"Notes/02random_effects/re.html\" \"Go to Random Effects Notes\"\n    click G \"Notes/03fixed_effects/fe.html\" \"Go to Fixed Effects Notes\"\n    click H \"Notes/04first_diff/fd.html\" \"Go to First Differences Notes\"\n    click E \"Notes/05gmm/gmm.html\" \"Go to GMM Notes\"\n    click I \"Notes/06anderson/ahsiao.html\" \"Go to Anderson-Hsiao Notes\"\n    click J \"Notes/06arellano/abond.html\" \"Go to Arellano-Bond Notes\"\n    click C \"Notes/07mle/mle.html\" \"Go to MLE Notes\"\n    click M \"Notes/08binary_choice/logit_probit.html\" \"Go to Binary Choice Notes\"\n    click O \"Notes/09multinomial/multinomial.html\" \"Go to Multinomial Choice Notes\"\n    click P \"Notes/10nested_logit/nested.html\" \"Go to Nested Logit Notes\"\n    click Q \"Notes/11ordered_response/ordered_response.html\" \"Go to Ordered Response Notes\"\n    click L \"Notes/12censored_truncated/tobit.html\" \"Go to Tobit Notes\"\n    click R \"Notes/13heckman/heckman.html\" \"Go to Heckman Notes\"\n    \n    click PS1 \"Problem_Sets/PS01/ps01.html\" \"Go to Problem Set 1\"\n    click PS2 \"Problem_Sets/PS02/ps02.html\" \"Go to Problem Set 2\"\n    click PS3 \"Problem_Sets/PS03/ps03.html\" \"Go to Problem Set 3\"\n\n\n    %% Classes\n    class A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R nav-node\n    class PS1,PS2,PS3 project-node\n\n    %% STYLE THE LINES\n    linkStyle 0 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 1 stroke:#00cc66,stroke-width:2px;\n    linkStyle 2 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 3 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 4 stroke:#00cc66,stroke-width:2px;\n    linkStyle 5 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 6 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 7 stroke:#00cc66,stroke-width:2px;\n    linkStyle 8 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 9 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 10 stroke:#00cc66,stroke-width:2px;\n    linkStyle 11 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 12 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 13 stroke:#00cc66,stroke-width:2px;\n    linkStyle 14 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 15 stroke:#00cc66,stroke-width:2px;\n    linkStyle 16 stroke:#ff4d4d,stroke-width:2px;"
  },
  {
    "objectID": "index.html#road-map",
    "href": "index.html#road-map",
    "title": "Econometrics: Panel Data",
    "section": "",
    "text": "flowchart TD\n    A(Panel Data) --&gt; B(Linear Models)\n    \n    B --&gt; D(Strict Exogeneity)\n    D --&gt; F(Random Effects)\n    D --&gt; G(Fixed Effects)\n    D --&gt; H(First Differences)\n    \n    B --&gt; E(Weak Exogeneity)\n    E --&gt; I(Anderson-Hsiao)\n    E --&gt; J(Arellano-Bond)\n    \n    \n    A --&gt; C(Maximum Likelihood)\n    \n    C --&gt; K(Discrete Choice)\n    K --&gt; M(Binary)\n    \n    K --&gt; N(Other)\n    N --&gt; O(Multinomial)\n    N --&gt; P(Nested Logit)\n    N --&gt; Q(Ordered Response)\n    \n    C --&gt; L(Tobit & Selection)\n    L --&gt; R(Heckman)\n    \n    \n    PS1([Problem Set 1])\n    PS2([Problem Set 2])\n    PS3([Problem Set 3])\n\n    %% Invisible link to nudge P near C/D without drawing a line\n    G ~~~ PS1\n    B ~~~ PS2\n    P ~~~ PS3\n\n    click D \"Notes/01pooled_ols/pols.html\" \"Go to Pooled OLS Notes\"\n    click F \"Notes/02random_effects/re.html\" \"Go to Random Effects Notes\"\n    click G \"Notes/03fixed_effects/fe.html\" \"Go to Fixed Effects Notes\"\n    click H \"Notes/04first_diff/fd.html\" \"Go to First Differences Notes\"\n    click E \"Notes/05gmm/gmm.html\" \"Go to GMM Notes\"\n    click I \"Notes/06anderson/ahsiao.html\" \"Go to Anderson-Hsiao Notes\"\n    click J \"Notes/06arellano/abond.html\" \"Go to Arellano-Bond Notes\"\n    click C \"Notes/07mle/mle.html\" \"Go to MLE Notes\"\n    click M \"Notes/08binary_choice/logit_probit.html\" \"Go to Binary Choice Notes\"\n    click O \"Notes/09multinomial/multinomial.html\" \"Go to Multinomial Choice Notes\"\n    click P \"Notes/10nested_logit/nested.html\" \"Go to Nested Logit Notes\"\n    click Q \"Notes/11ordered_response/ordered_response.html\" \"Go to Ordered Response Notes\"\n    click L \"Notes/12censored_truncated/tobit.html\" \"Go to Tobit Notes\"\n    click R \"Notes/13heckman/heckman.html\" \"Go to Heckman Notes\"\n    \n    click PS1 \"Problem_Sets/PS01/ps01.html\" \"Go to Problem Set 1\"\n    click PS2 \"Problem_Sets/PS02/ps02.html\" \"Go to Problem Set 2\"\n    click PS3 \"Problem_Sets/PS03/ps03.html\" \"Go to Problem Set 3\"\n\n\n    %% Classes\n    class A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R nav-node\n    class PS1,PS2,PS3 project-node\n\n    %% STYLE THE LINES\n    linkStyle 0 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 1 stroke:#00cc66,stroke-width:2px;\n    linkStyle 2 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 3 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 4 stroke:#00cc66,stroke-width:2px;\n    linkStyle 5 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 6 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 7 stroke:#00cc66,stroke-width:2px;\n    linkStyle 8 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 9 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 10 stroke:#00cc66,stroke-width:2px;\n    linkStyle 11 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 12 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 13 stroke:#00cc66,stroke-width:2px;\n    linkStyle 14 stroke:#ff4d4d,stroke-width:2px;\n    linkStyle 15 stroke:#00cc66,stroke-width:2px;\n    linkStyle 16 stroke:#ff4d4d,stroke-width:2px;"
  },
  {
    "objectID": "index.html#class-notes",
    "href": "index.html#class-notes",
    "title": "Econometrics: Panel Data",
    "section": "Class Notes",
    "text": "Class Notes\n\n\n\n\n\n\n\n\nPooled OLS\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Effects\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Differences\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Method of Moments Estimators\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nAnderson-Hsiao Estimator\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nArellano Bond\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimators\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nBinary Choice Models\n\n1 min\n\nLogit & Probit Models\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Models\n\n11 min\n\nConditional Logit & Multinomial Logit Models\n\n\n\n\n\n\n\n\n\n\n\n\nOrdered Response Models\n\n4 min\n\nLatent Variable Models: Unobserved Outcome Variable\n\n\n\n\n\n\n\n\n\n\n\n\nTobit Model\n\n7 min\n\nDealing with Unobserved Data: Censoring & Truncation\n\n\n\n\n\n\n\n\n\n\nHeckit Estimator\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested Models\n\n7 min\n\nNested Logit & Multinomial Probit\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#problem-sets",
    "href": "index.html#problem-sets",
    "title": "Econometrics: Panel Data",
    "section": "Problem Sets",
    "text": "Problem Sets\n\n\n\n\n\n\n\n\nProblem Set 02\n\n6 min\n\nLinear Models: Fixed Effects, First-Diffs, & Arellano-Bond\n\n\n\n\n\n\n\n\n\n\nProblem Set 03\n\n1 min\n\nMLE: Binary choice, mLogit, oProbit, & Tobit\n\n\n\n\n\n\n\n\n\n\nProblem Set 01\n\n1 min\n\nLinear Models: Pooled OLS, Random Effects, Fixed Effects, & First Diffs\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Econometrics: Panel Data",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\n\nProject 01\n\n1 min\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Notes/01pooled_ols/pols.html",
    "href": "Notes/01pooled_ols/pols.html",
    "title": "Pooled OLS",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Notes/03fixed_effects/fe.html",
    "href": "Notes/03fixed_effects/fe.html",
    "title": "Fixed Effects",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Notes/05gmm/gmm.html",
    "href": "Notes/05gmm/gmm.html",
    "title": "Generalized Method of Moments Estimators",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Notes/06arellano/abond.html",
    "href": "Notes/06arellano/abond.html",
    "title": "Arellano Bond",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Notes/08binary_choice/logit_probit.html",
    "href": "Notes/08binary_choice/logit_probit.html",
    "title": "Binary Choice Models",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Notes/10nested_logit/nested.html",
    "href": "Notes/10nested_logit/nested.html",
    "title": "Nested Models",
    "section": "",
    "text": "Recall that from our multinomial models that they only hold under the IID alternatives assumption; this tells us that alternatives are completely independent of each other, each being drawn randomly with equal chance. Of course, we can think of immediate examples that would violate this assumption: cities such as Barcelona and Madrid are likely correlated; they share similar characteristics in language, culture, food, etc. Thus, deciding to move to one of these cities is surely influenced by each cities’ similarities.\nIndirectly, this IID assumption leads to the “Independence of Irrelevant Alternatives” assumption, derived from the fact that relative probabilities for any two alternatives depend only on the attributes of those two alternatives. This implies that adding another alternative or changing the characteristics of a third does not affect the relative odds between any two alternatives \\(j\\) and \\(\\ell\\).\n\n\n\nTo relax this assumption, we are going to allow for some correlation between errors.\nTo understand the Nested Logit model, it’s helpful to look at a diagram to understand the structure behind it. Let’s imagine an example of how and individual decides to choose to pursue a Master’s in Economics. The first decision you might make is the location: will you study at a university that is abroad or domestic? This first decision split consists of the “limbs”. From there, we make another nested decision between the universities themselves: these final points of our tree are the branches.\n\n\n\n\n\nflowchart TD\n  %% 1. Define a \"plain\" style: No border (stroke-width:0px), white background\n  classDef plain fill:#fff,stroke-width:0px,color:#000;\n\n  A{Masters in Economics} --&gt; |Limb 1|B{Abroad}\n  A --&gt;|Limb 2| C{Home}\n  B --&gt;|Branch 1| D{BSE}\n  B --&gt;|Branch 2| E{Carlos III}\n  C --&gt;|Branch 1| F{UNC}\n  C --&gt;|Branch 2| G{Duke}\n\n  %% 2. Create the \"Text\" nodes below using HTML for math\n  %% We use `---` to connect them, which creates a simple line\n  D --- D_text[\"V&lt;sub&gt;11&lt;/sub&gt; + &epsilon;&lt;sub&gt;11&lt;/sub&gt;\"]\n  E --- E_text[\"V&lt;sub&gt;12&lt;/sub&gt; + &epsilon;&lt;sub&gt;12&lt;/sub&gt;\"]\n  F --- F_text[\"V&lt;sub&gt;21&lt;/sub&gt; + &epsilon;&lt;sub&gt;21&lt;/sub&gt;\"]\n  G --- G_text[\"V&lt;sub&gt;22&lt;/sub&gt; + &epsilon;&lt;sub&gt;22&lt;/sub&gt;\"]\n\n  %% 3. Apply the \"plain\" style to your text nodes\n  class D_text,E_text,F_text,G_text plain\n\n\n\n\n\n\n\n\nIn our previous multinomial model, we assumed that these branches within the same branches were uncorrelated; now, we allow for same-branch limbs to be correlated. However, it is important to note that there still is no correlation across limbs; i.e., \\(Corr(\\varepsilon_{11},\\varepsilon_{21}) = 0\\). In our above example, this means that BSE and Carlos III are allow to be correlated in our model; i.e., \\(Corr(\\varepsilon_{11},\\varepsilon_{12})\\neq0\\). This is a reasonable relaxation; we would expect both universities to share similar characteristics in language, professional network, grading scheme, etc.\nA major challenge with these types of nested models is the decision of structure itself; there are many ways we can construct this decision tree. For example, perhaps our first choice is based on cost; then on rankings, then faculty, then location… etc.\nTo relax this assumption of IID alternatives, rather than assuming \\(\\varepsilon_i \\sim \\mathcal{Gumbel}\\) as we did before, we now assume that the errors follow a General Extreme Value Distribution; formally, \\[\\varepsilon_i \\sim GEV\\]\nWhere the GEV CDF is written as the following: \\[F(\\varepsilon) = \\exp \\left( - \\sum_{m=1}^{K} \\left( \\sum_{\\ell=1}^{K_m} e^{\\frac{-\\varepsilon_{m\\ell}}{\\rho_m}} \\right)^{\\rho_m} \\right)\\]\n\n\n\nPlugging this distribution into our joint pdf of the errors (integral), this simplifies into:\n\\[\nP_{jk}\n= \\Pr(y_i = jk \\mid X_i)\n=\n\\left[\n\\underbrace{\n\\frac{\\exp\\!\\left( V_{jk} / \\rho_m \\right)}\n{\\sum\\limits_{k \\in m} \\exp\\!\\left( V_{jk} / \\rho_m \\right)}\n}_{\\text{within-nest choice}}\n\\;\\times\\;\n\\underbrace{\n\\frac{\n\\left( \\sum\\limits_{k \\in m} \\exp\\!\\left( V_{jk} / \\rho_m \\right) \\right)^{\\rho_m}\n}{\n\\sum\\limits_{\\ell=1}^{M}\n\\left( \\sum\\limits_{k \\in \\ell} \\exp\\!\\left( V_{jk} / \\rho_\\ell \\right) \\right)^{\\rho_\\ell}\n}\n}_{\\text{nest choice}}\n\\right]\n\\]\nWhere the within-nest choice is a logit model conditional on being in nest m, where we choose the best option available. The second term is the nest choice, where we decide the nest structure that has the best option inside each nest, on average.\nEssentially, we are decomposing the choice probability into these two components, seen below:\n\n\n\nSuppose there are \\(J\\) limbs to choose from. The \\(j^{th}\\) limb has \\(K_j\\) branches: \\(j1, ..., jk,...,jK_j\\)\nThe utility for the alternative in the kth branch of the jth limb is:\n\\[U_{jk} = V_{jk} + \\varepsilon_{jk}\\] where \\(k = 1, 2, ..., K_j\\) and \\(j = 1, 2, ..., J\\). For a model with this nesting, \\(p_{jk}\\) , the joint probability of being on limb \\(j\\) and branch \\(k\\) can be factored as \\(p_j\\) , the probability of choosing limb \\(j\\), times \\(p_{k \\mid j}\\) , the probability of choosing branch \\(k\\) conditional on being on limb \\(j\\):\n\\[\nP_{jk}=P_j × P_{k \\mid i}\n\\]\nThis get rewritten as:\n\\[\nP_{jk}\n=\n\\Pr(y_i = jk \\mid X_i)\n=\n\\left[\n\\frac{\ne^{V_{jk}}\n\\;\n\\frac{\\partial}{\\partial e^{V_{jk}}}\n\\left[\n\\sum_{m=1}^{K}\n\\left(\n\\sum_{\\ell =1}^{K_m}\n\\left(e^{V_{\\ell m}}\\right)^{1/\\rho_m}\n\\right)^{\\rho_m}\n\\right]\n}{\n\\sum_{m=1}^{K}\n\\left(\n\\sum_{\\ell = 1}^{K_m}\n\\left(e^{V_{\\ell m}}\\right)^{1/\\rho_m}\n\\right)^{\\rho_m}\n}\n\\right]\n\\]\nWhere \\(\\rho_m\\) is defined as our “scale parameter”, measuring correlation b/w same-limb errors\n\\[\n\\rho_m = \\sqrt{1-Corr(\\varepsilon_{m_\\ell},\\varepsilon_{mk})} \\in [0,1] \\\\\n\\rho_m = \\begin{cases}\n   1 &\\text{if no correlation b/w errors} \\\\\n   0 &\\text{if perfect correlation b/w errors}\n\\end{cases}\n\\]\nNote that if \\(\\rho_m =1\\), this formula collapses back down into the multinomial logit form from before.\n\n\n\nSuppose we estimate the following model:\n\\[V_{jk} = Z_j'\\alpha + X_{jk}'\\beta_j\\]\nWhere \\(Z_j\\) are our limb-specific regressors, i.e. varies over limbs only, and \\(X_{jk}\\) varies both over limbs AND branches. \\(\\alpha, \\beta\\) are the regression parameters.\nOur next step is to maximize \\(p_{jk} = p_j \\times p_{k \\mid j}\\) w.r.t. \\(\\alpha, \\beta, \\text{and } \\rho\\). Thus, we plug in our value for \\(V_{jk}\\) into our \\(p_{jk}\\) expression.\nThe joint distribution yields the nested logit model:\n\\[\nP_{jk} = P_j \\times P_{k|j} = \\frac{\\exp(z'_j \\alpha + \\rho_j I_j)}{\\sum_{m=1}^{J} \\exp(z'_m \\alpha + \\rho_m I_m)} \\times \\frac{\\exp(x'_{jk} \\beta_j / \\rho_j)}{\\sum_{l=1}^{K_j} \\exp(x'_{jl} \\beta_j / \\rho_j)}\n\\]\nwhere the so-called inclusive value \\(I_j\\), is defined as:\n\\[\nI_j = \\ln \\left( \\sum_{l=1}^{K_j} \\exp(x'_{jl} \\beta_j / \\rho_j) \\right)\n\\]\nNote that this also works for regressors that do not vary over alternatives. In this case, \\(V_{jk} = z′\\alpha_j + x′\\beta_{jk}\\), and we must normalize one of the \\(\\beta_{jk}\\) for identification.\n\n\n\nTo estimate the model parameters, we employ Maximum Likelihood Estimation (MLE).\n1. Data Structure\nLet \\(y_{ijk}\\) be an indicator variable for the observed choice of the \\(i\\)-th individual.\n\\[\ny_{ijk} =\n\\begin{cases}\n1 & \\text{if individual } i \\text{ chooses alternative } k \\text{ in nest } j \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n2. Joint Probability\nThe probability of observing a specific choice is the product of the marginal probability of choosing the nest (limb) and the conditional probability of choosing the alternative (branch) within that nest:\n\\[\np_{ijk} = p_{ij} \\times p_{ik|j}\n\\]\n3. The Likelihood Function\nFor a single observation \\(y_i\\), the probability mass function combines the choice probabilities with the observed indicator variables. Since \\(y_{ijk}=0\\) for non-chosen alternatives (and \\(p^0=1\\)), only the chosen alternative contributes to the likelihood:\n\\[\nf(y_i) = \\prod_{j=1}^{J} \\prod_{k=1}^{K_j} (p_{ij} \\times p_{ik|j})^{y_{ijk}}\n\\]\n4. The Log-Likelihood Function\nWe maximize the log-likelihood function, \\(l(\\alpha, \\beta, \\rho)\\). Taking the natural logarithm allows us to decompose the estimation into two additive components: the choice of the nest and the choice within the nest.\n\\[\n\\ell(\\alpha, \\beta, \\rho) = \\sum_{i=1}^{N} \\left( \\underbrace{\\sum_{j=1}^{J} y_{ij} \\ln p_{ij}}_{\\text{Limb Choice}} + \\underbrace{\\sum_{j=1}^{J} \\sum_{k=1}^{K_j} y_{ijk} \\ln p_{ik|j}}_{\\text{Branch Choice}} \\right)\n\\]\nThe solution to this maximization problem yields our estimators:\n\\[\n(\\hat{\\alpha}, \\hat{\\beta}, \\hat{\\rho}) = \\operatorname*{argmax}_{\\alpha, \\beta, \\rho} \\ l(\\alpha, \\beta, \\rho)\n\\]\n\n\n\n\nSimilar motivations and logic as Nested Logit, with the only different being the form of the error distribution. As with regular probit regression, we assumed the errors to follow a normal distribution; more formally,\n\\[\\varepsilon_i \\sim \\mathcal{N}(0, \\Sigma)\\]\nWhere \\(\\varepsilon\\) is a \\((mx1)\\) matrix, and\n\\[\n\\Sigma =\n\\begin{bmatrix}\n  \\begin{bmatrix}\n    - & - \\\\\n    - & -\n  \\end{bmatrix} & 0 \\\\\n  0 & \\begin{bmatrix}\n    - & - \\\\\n    - & -\n  \\end{bmatrix}\n\\end{bmatrix}\n\\]\nSuch that the variance of the errors allows for a nesting structure: errors amongst same-branch observations are allowed to be correlated (diagonal), but NOT across limbs (off-diagonal). This is very similar to the clustering standard errors from the heteroskedasticity relaxation from our random/fixed effects models.\nIssue: Multinomial Probit choice probability does not have a closed-form solution; for an \\(m\\) choice problem, we have to solve for a \\(m-1\\) fold integral; computationally very expensive as we move beyond 5 or so alternatives."
  },
  {
    "objectID": "Notes/10nested_logit/nested.html#motivation",
    "href": "Notes/10nested_logit/nested.html#motivation",
    "title": "Nested Models",
    "section": "",
    "text": "Recall that from our multinomial models that they only hold under the IID alternatives assumption; this tells us that alternatives are completely independent of each other, each being drawn randomly with equal chance. Of course, we can think of immediate examples that would violate this assumption: cities such as Barcelona and Madrid are likely correlated; they share similar characteristics in language, culture, food, etc. Thus, deciding to move to one of these cities is surely influenced by each cities’ similarities.\nIndirectly, this IID assumption leads to the “Independence of Irrelevant Alternatives” assumption, derived from the fact that relative probabilities for any two alternatives depend only on the attributes of those two alternatives. This implies that adding another alternative or changing the characteristics of a third does not affect the relative odds between any two alternatives \\(j\\) and \\(\\ell\\)."
  },
  {
    "objectID": "Notes/10nested_logit/nested.html#nested-logit",
    "href": "Notes/10nested_logit/nested.html#nested-logit",
    "title": "Nested Models",
    "section": "",
    "text": "To relax this assumption, we are going to allow for some correlation between errors.\nTo understand the Nested Logit model, it’s helpful to look at a diagram to understand the structure behind it. Let’s imagine an example of how and individual decides to choose to pursue a Master’s in Economics. The first decision you might make is the location: will you study at a university that is abroad or domestic? This first decision split consists of the “limbs”. From there, we make another nested decision between the universities themselves: these final points of our tree are the branches.\n\n\n\n\n\nflowchart TD\n  %% 1. Define a \"plain\" style: No border (stroke-width:0px), white background\n  classDef plain fill:#fff,stroke-width:0px,color:#000;\n\n  A{Masters in Economics} --&gt; |Limb 1|B{Abroad}\n  A --&gt;|Limb 2| C{Home}\n  B --&gt;|Branch 1| D{BSE}\n  B --&gt;|Branch 2| E{Carlos III}\n  C --&gt;|Branch 1| F{UNC}\n  C --&gt;|Branch 2| G{Duke}\n\n  %% 2. Create the \"Text\" nodes below using HTML for math\n  %% We use `---` to connect them, which creates a simple line\n  D --- D_text[\"V&lt;sub&gt;11&lt;/sub&gt; + &epsilon;&lt;sub&gt;11&lt;/sub&gt;\"]\n  E --- E_text[\"V&lt;sub&gt;12&lt;/sub&gt; + &epsilon;&lt;sub&gt;12&lt;/sub&gt;\"]\n  F --- F_text[\"V&lt;sub&gt;21&lt;/sub&gt; + &epsilon;&lt;sub&gt;21&lt;/sub&gt;\"]\n  G --- G_text[\"V&lt;sub&gt;22&lt;/sub&gt; + &epsilon;&lt;sub&gt;22&lt;/sub&gt;\"]\n\n  %% 3. Apply the \"plain\" style to your text nodes\n  class D_text,E_text,F_text,G_text plain\n\n\n\n\n\n\n\n\nIn our previous multinomial model, we assumed that these branches within the same branches were uncorrelated; now, we allow for same-branch limbs to be correlated. However, it is important to note that there still is no correlation across limbs; i.e., \\(Corr(\\varepsilon_{11},\\varepsilon_{21}) = 0\\). In our above example, this means that BSE and Carlos III are allow to be correlated in our model; i.e., \\(Corr(\\varepsilon_{11},\\varepsilon_{12})\\neq0\\). This is a reasonable relaxation; we would expect both universities to share similar characteristics in language, professional network, grading scheme, etc.\nA major challenge with these types of nested models is the decision of structure itself; there are many ways we can construct this decision tree. For example, perhaps our first choice is based on cost; then on rankings, then faculty, then location… etc.\nTo relax this assumption of IID alternatives, rather than assuming \\(\\varepsilon_i \\sim \\mathcal{Gumbel}\\) as we did before, we now assume that the errors follow a General Extreme Value Distribution; formally, \\[\\varepsilon_i \\sim GEV\\]\nWhere the GEV CDF is written as the following: \\[F(\\varepsilon) = \\exp \\left( - \\sum_{m=1}^{K} \\left( \\sum_{\\ell=1}^{K_m} e^{\\frac{-\\varepsilon_{m\\ell}}{\\rho_m}} \\right)^{\\rho_m} \\right)\\]\n\n\n\nPlugging this distribution into our joint pdf of the errors (integral), this simplifies into:\n\\[\nP_{jk}\n= \\Pr(y_i = jk \\mid X_i)\n=\n\\left[\n\\underbrace{\n\\frac{\\exp\\!\\left( V_{jk} / \\rho_m \\right)}\n{\\sum\\limits_{k \\in m} \\exp\\!\\left( V_{jk} / \\rho_m \\right)}\n}_{\\text{within-nest choice}}\n\\;\\times\\;\n\\underbrace{\n\\frac{\n\\left( \\sum\\limits_{k \\in m} \\exp\\!\\left( V_{jk} / \\rho_m \\right) \\right)^{\\rho_m}\n}{\n\\sum\\limits_{\\ell=1}^{M}\n\\left( \\sum\\limits_{k \\in \\ell} \\exp\\!\\left( V_{jk} / \\rho_\\ell \\right) \\right)^{\\rho_\\ell}\n}\n}_{\\text{nest choice}}\n\\right]\n\\]\nWhere the within-nest choice is a logit model conditional on being in nest m, where we choose the best option available. The second term is the nest choice, where we decide the nest structure that has the best option inside each nest, on average.\nEssentially, we are decomposing the choice probability into these two components, seen below:\n\n\n\nSuppose there are \\(J\\) limbs to choose from. The \\(j^{th}\\) limb has \\(K_j\\) branches: \\(j1, ..., jk,...,jK_j\\)\nThe utility for the alternative in the kth branch of the jth limb is:\n\\[U_{jk} = V_{jk} + \\varepsilon_{jk}\\] where \\(k = 1, 2, ..., K_j\\) and \\(j = 1, 2, ..., J\\). For a model with this nesting, \\(p_{jk}\\) , the joint probability of being on limb \\(j\\) and branch \\(k\\) can be factored as \\(p_j\\) , the probability of choosing limb \\(j\\), times \\(p_{k \\mid j}\\) , the probability of choosing branch \\(k\\) conditional on being on limb \\(j\\):\n\\[\nP_{jk}=P_j × P_{k \\mid i}\n\\]\nThis get rewritten as:\n\\[\nP_{jk}\n=\n\\Pr(y_i = jk \\mid X_i)\n=\n\\left[\n\\frac{\ne^{V_{jk}}\n\\;\n\\frac{\\partial}{\\partial e^{V_{jk}}}\n\\left[\n\\sum_{m=1}^{K}\n\\left(\n\\sum_{\\ell =1}^{K_m}\n\\left(e^{V_{\\ell m}}\\right)^{1/\\rho_m}\n\\right)^{\\rho_m}\n\\right]\n}{\n\\sum_{m=1}^{K}\n\\left(\n\\sum_{\\ell = 1}^{K_m}\n\\left(e^{V_{\\ell m}}\\right)^{1/\\rho_m}\n\\right)^{\\rho_m}\n}\n\\right]\n\\]\nWhere \\(\\rho_m\\) is defined as our “scale parameter”, measuring correlation b/w same-limb errors\n\\[\n\\rho_m = \\sqrt{1-Corr(\\varepsilon_{m_\\ell},\\varepsilon_{mk})} \\in [0,1] \\\\\n\\rho_m = \\begin{cases}\n   1 &\\text{if no correlation b/w errors} \\\\\n   0 &\\text{if perfect correlation b/w errors}\n\\end{cases}\n\\]\nNote that if \\(\\rho_m =1\\), this formula collapses back down into the multinomial logit form from before.\n\n\n\nSuppose we estimate the following model:\n\\[V_{jk} = Z_j'\\alpha + X_{jk}'\\beta_j\\]\nWhere \\(Z_j\\) are our limb-specific regressors, i.e. varies over limbs only, and \\(X_{jk}\\) varies both over limbs AND branches. \\(\\alpha, \\beta\\) are the regression parameters.\nOur next step is to maximize \\(p_{jk} = p_j \\times p_{k \\mid j}\\) w.r.t. \\(\\alpha, \\beta, \\text{and } \\rho\\). Thus, we plug in our value for \\(V_{jk}\\) into our \\(p_{jk}\\) expression.\nThe joint distribution yields the nested logit model:\n\\[\nP_{jk} = P_j \\times P_{k|j} = \\frac{\\exp(z'_j \\alpha + \\rho_j I_j)}{\\sum_{m=1}^{J} \\exp(z'_m \\alpha + \\rho_m I_m)} \\times \\frac{\\exp(x'_{jk} \\beta_j / \\rho_j)}{\\sum_{l=1}^{K_j} \\exp(x'_{jl} \\beta_j / \\rho_j)}\n\\]\nwhere the so-called inclusive value \\(I_j\\), is defined as:\n\\[\nI_j = \\ln \\left( \\sum_{l=1}^{K_j} \\exp(x'_{jl} \\beta_j / \\rho_j) \\right)\n\\]\nNote that this also works for regressors that do not vary over alternatives. In this case, \\(V_{jk} = z′\\alpha_j + x′\\beta_{jk}\\), and we must normalize one of the \\(\\beta_{jk}\\) for identification.\n\n\n\nTo estimate the model parameters, we employ Maximum Likelihood Estimation (MLE).\n1. Data Structure\nLet \\(y_{ijk}\\) be an indicator variable for the observed choice of the \\(i\\)-th individual.\n\\[\ny_{ijk} =\n\\begin{cases}\n1 & \\text{if individual } i \\text{ chooses alternative } k \\text{ in nest } j \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n2. Joint Probability\nThe probability of observing a specific choice is the product of the marginal probability of choosing the nest (limb) and the conditional probability of choosing the alternative (branch) within that nest:\n\\[\np_{ijk} = p_{ij} \\times p_{ik|j}\n\\]\n3. The Likelihood Function\nFor a single observation \\(y_i\\), the probability mass function combines the choice probabilities with the observed indicator variables. Since \\(y_{ijk}=0\\) for non-chosen alternatives (and \\(p^0=1\\)), only the chosen alternative contributes to the likelihood:\n\\[\nf(y_i) = \\prod_{j=1}^{J} \\prod_{k=1}^{K_j} (p_{ij} \\times p_{ik|j})^{y_{ijk}}\n\\]\n4. The Log-Likelihood Function\nWe maximize the log-likelihood function, \\(l(\\alpha, \\beta, \\rho)\\). Taking the natural logarithm allows us to decompose the estimation into two additive components: the choice of the nest and the choice within the nest.\n\\[\n\\ell(\\alpha, \\beta, \\rho) = \\sum_{i=1}^{N} \\left( \\underbrace{\\sum_{j=1}^{J} y_{ij} \\ln p_{ij}}_{\\text{Limb Choice}} + \\underbrace{\\sum_{j=1}^{J} \\sum_{k=1}^{K_j} y_{ijk} \\ln p_{ik|j}}_{\\text{Branch Choice}} \\right)\n\\]\nThe solution to this maximization problem yields our estimators:\n\\[\n(\\hat{\\alpha}, \\hat{\\beta}, \\hat{\\rho}) = \\operatorname*{argmax}_{\\alpha, \\beta, \\rho} \\ l(\\alpha, \\beta, \\rho)\n\\]"
  },
  {
    "objectID": "Notes/10nested_logit/nested.html#multinomial-probit",
    "href": "Notes/10nested_logit/nested.html#multinomial-probit",
    "title": "Nested Models",
    "section": "",
    "text": "Similar motivations and logic as Nested Logit, with the only different being the form of the error distribution. As with regular probit regression, we assumed the errors to follow a normal distribution; more formally,\n\\[\\varepsilon_i \\sim \\mathcal{N}(0, \\Sigma)\\]\nWhere \\(\\varepsilon\\) is a \\((mx1)\\) matrix, and\n\\[\n\\Sigma =\n\\begin{bmatrix}\n  \\begin{bmatrix}\n    - & - \\\\\n    - & -\n  \\end{bmatrix} & 0 \\\\\n  0 & \\begin{bmatrix}\n    - & - \\\\\n    - & -\n  \\end{bmatrix}\n\\end{bmatrix}\n\\]\nSuch that the variance of the errors allows for a nesting structure: errors amongst same-branch observations are allowed to be correlated (diagonal), but NOT across limbs (off-diagonal). This is very similar to the clustering standard errors from the heteroskedasticity relaxation from our random/fixed effects models.\nIssue: Multinomial Probit choice probability does not have a closed-form solution; for an \\(m\\) choice problem, we have to solve for a \\(m-1\\) fold integral; computationally very expensive as we move beyond 5 or so alternatives."
  },
  {
    "objectID": "Notes/12censored_truncated/tobit.html",
    "href": "Notes/12censored_truncated/tobit.html",
    "title": "Tobit Model",
    "section": "",
    "text": "When working with panel or cross-sectional data, we often encounter situations where data is incompletely observed. It is crucial to distinguish between two types:\n\n\n\nCensored Data: Information is lost on the dependent variable (\\(y\\)), but we have full information on the regressors (\\(x\\)) for the entire sample.\n\nExample: Income surveys where high earners are recorded simply as “$100k+”. We know their age, education, etc., but their exact income is “right-censored.”\nKey Feature: We have a representative sample of the population, but the outcome value is “stuck” at a threshold for some.\n\nTruncated Data: We do not have the full sample. Data is lost on both the dependent variable and the regressors because the observation rule depends on \\(y\\).\n\nExample: Studying determinants of poverty but only sampling people below the poverty line. We completely lack data (both \\(y\\) and \\(x\\)) for anyone above the line.\nKey Feature: The sample is a non-representative sub-sample of the population.\n\n\n\n\n\n\n\nTo model these processes, we assume there is an underlying “Latent Variable” \\(y^*\\) that satisfies the classical linear assumptions:\n\\[\ny_i^* = x_i' \\beta + \\varepsilon_i\n\\]\nHowever, \\(y^*\\) is not perfectly observed.\n\n\n1. Censoring from Below (Left-Censoring)\nWe observe \\(y_i\\) based on a threshold \\(L\\):\n\\[\ny_i = \\begin{cases}\ny_i^* & \\text{if } y_i^* &gt; L \\\\\nL & \\text{if } y_i^* \\leq L\n\\end{cases}\n\\]\nResult: A “spike” (mass point) in the distribution of \\(y\\) at \\(L\\).\n2. Censoring from Above (Right-Censoring)\n\\[\ny_i = \\begin{cases}\ny_i^* & \\text{if } y_i^* &lt; U \\\\\nU & \\text{if } y_i^* \\geq U\n\\end{cases}\n\\]\n3. Truncation from Below\nWe only observe unit \\(i\\) if \\(y_i^* &gt; L\\).\n\\[\n\\text{Sample Rule: Keep } i \\text{ if } y_i^* &gt; L\n\\]\nResult: The distribution is “cut off.” The area under the curve must sum to 1, so the remaining probability density is scaled up.\n\n\n\n\nIf we run a standard OLS regression (\\(y\\) on \\(x\\)) using censored data, our estimates will be biased.\n\n\nAt low values of \\(x\\), many observations hit the lower bound \\(L\\). This flattens the slope of the observed relationship relative to the true latent relationship.\n\n\n\n\n\n\n\n\n\n\n\n\nThe expected value of the observed data is:\n\\[\nE[y|x] = P(y^* \\le L) \\cdot L + P(y^* &gt; L) \\cdot E[y^* | y^* &gt; L, x]\n\\]\nThis relationship is non-linear.\n\nAt high \\(x\\), \\(E[y|x] \\approx E[y^*|x]\\) (convergence to true slope).\nAt low \\(x\\), data piles up at \\(L\\), dragging the expectation line flat.\nConsequence: OLS will generally underestimate the slope coefficient \\(\\beta\\).\n\n\n\n\n\n\nSince OLS is inconsistent, we must use Maximum Likelihood Estimation (MLE). We need to make parametric assumptions (usually Normality) about the error term: \\(\\varepsilon \\sim N(0, \\sigma^2)\\).\n\n\nThe density of observed \\(y\\) is a mixture of a continuous part and a discrete probability mass.\n\nContinuous part (\\(y_i &gt; L\\)): We observe the actual value. The likelihood contribution is the PDF: \\(f^*(y_i | x_i)\\).\nCensored part (\\(y_i = L\\)): We know only that \\(y^* \\le L\\). The likelihood contribution is the probability (CDF): \\(F^*(L | x_i)\\).\n\nWe define a dummy indicator \\(d_i\\):\n\\[\nd_i = \\begin{cases}\n1 & \\text{if } y_i &gt; L \\quad (\\text{Uncensored}) \\\\\n0 & \\text{if } y_i = L \\quad (\\text{Censored})\n\\end{cases}\n\\]\nThe Log-Likelihood Function:\n\\[\n\\ell(\\beta, \\sigma^2) = \\sum_{i=1}^N \\left[ d_i \\ln \\left( \\frac{1}{\\sigma} \\Phi\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) \\right) + (1-d_i) \\ln \\Phi\\left(\\frac{L - x_i'\\beta}{\\sigma}\\right) \\right]\n\\]\n\n\n\nHere, we do not observe anyone with \\(y^* \\le L\\). We must re-normalize the probability density so it integrates to 1 over the range \\((L, \\infty)\\).\n\\[\nf(y_i | y_i &gt; L, x_i) = \\frac{f^*(y_i|x_i)}{P(y_i &gt; L | x_i)} = \\frac{\\frac{1}{\\sigma}\\phi(\\frac{y_i - x_i'\\beta}{\\sigma})}{1 - \\Phi(\\frac{L - x_i'\\beta}{\\sigma})}\n\\]\nThe Log-Likelihood Function:\n\\[\n\\ell = \\sum_{i=1}^N \\left[ \\ln \\left( \\frac{1}{\\sigma}\\Phi(\\cdot) \\right) - \\ln \\left( 1 - \\Phi\\left(\\frac{L - x_i'\\beta}{\\sigma}\\right) \\right) \\right]\n\\]\nIntuition: In the truncated case, the denominator \\(1 - \\Phi(\\cdot)\\) essentially tells the model: “Hey, we are missing the left tail of the bell curve, so inflate the probabilities of the data we do see to account for that missing mass.”\n\n\n\n\nThe Tobit model is the specific and most common case of censored regression where the threshold is zero (\\(L=0\\)) and censoring is from below.\n\n\n\nLatent Variable: \\(y_i^* = x_i'\\beta + \\varepsilon_i\\), with \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\).\nObservation Rule:\n\n\\[\ny_i = \\max(0, y_i^*)\n\\]\n\n\n\nA classic example is hours worked (\\(H_i\\)).\n\n\\(H_i^*\\): Desired hours (latent utility). A person might desire “-5 hours” of work (meaning they need a high wage just to start working).\n\\(H_i\\): Actual observed hours. Since you can’t work negative hours, we observe 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe split the sample into those working (\\(H_i &gt; 0\\)) and those not working (\\(H_i = 0\\)).\n\nFor \\(H_i &gt; 0\\) (Uncensored): Contribution is the density of the normal distribution:\n\n\\[\n\\frac{1}{\\sigma} \\Phi\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right)\n\\]\n\nFor \\(H_i = 0\\) (Censored): Contribution is the probability that latent utility is negative:\n\n\\[\n\\mathbb{P}(y_i^* \\le 0) = \\mathbb{P}\\left(\\frac{y_i^* - x_i'\\beta}{\\sigma} \\le \\frac{-x_i'\\beta}{\\sigma}\\right) = \\Phi\\left(-\\frac{x_i'\\beta}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\n\\]\nCombined Log-Likelihood:\n\\[\n\\ell(\\beta, \\sigma) = \\sum_{y_i = 0} \\ln \\left[ 1 - \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right) \\right] + \\sum_{y_i &gt; 0} \\ln \\left[ \\frac{1}{\\sigma} \\phi\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) \\right]\n\\]\n\n\n\n\nIdeally, we estimate \\(\\beta\\) and \\(\\sigma^2\\) separately.\nHowever, if there is high multicollinearity or little variation in the censored portion, it can be difficult to disentangle \\(\\beta\\) from \\(\\sigma\\).\nHomoskedasticity Assumption: Crucial for Tobit. If \\(\\text{Var}(\\varepsilon)\\) varies with \\(x\\), the Tobit estimates are inconsistent (unlike OLS, where heteroskedasticity only affects standard errors).\n\n\n\n\nIn Tobit, there are two different marginal effects of interest:\n\nEffect on the Latent Variable:\n\n\\[\n\\frac{\\partial E[y^*|x]}{\\partial x_k} = \\beta_k\n\\]\nInterpretation: How a change in \\(x\\) changes “desired” hours.\n\nEffect on the Observed Variable:\n\n\\[\n\\frac{\\partial E[y|x]}{\\partial x_k} = \\beta_k \\cdot \\Phi\\left(\\frac{x'\\beta}{\\sigma}\\right)\n\\]\nInterpretation: How a change in \\(x\\) changes actual hours worked.\nNote: The effect on actual hours is always smaller than the effect on desired hours because \\(0 \\le \\Phi(\\cdot) \\le 1\\). This dampening factor \\(\\Phi\\) represents the probability of being uncensored (actually working)."
  },
  {
    "objectID": "Notes/12censored_truncated/tobit.html#dealing-with-unobserved-data",
    "href": "Notes/12censored_truncated/tobit.html#dealing-with-unobserved-data",
    "title": "Tobit Model",
    "section": "",
    "text": "When working with panel or cross-sectional data, we often encounter situations where data is incompletely observed. It is crucial to distinguish between two types:\n\n\n\nCensored Data: Information is lost on the dependent variable (\\(y\\)), but we have full information on the regressors (\\(x\\)) for the entire sample.\n\nExample: Income surveys where high earners are recorded simply as “$100k+”. We know their age, education, etc., but their exact income is “right-censored.”\nKey Feature: We have a representative sample of the population, but the outcome value is “stuck” at a threshold for some.\n\nTruncated Data: We do not have the full sample. Data is lost on both the dependent variable and the regressors because the observation rule depends on \\(y\\).\n\nExample: Studying determinants of poverty but only sampling people below the poverty line. We completely lack data (both \\(y\\) and \\(x\\)) for anyone above the line.\nKey Feature: The sample is a non-representative sub-sample of the population."
  },
  {
    "objectID": "Notes/12censored_truncated/tobit.html#the-latent-variable-model",
    "href": "Notes/12censored_truncated/tobit.html#the-latent-variable-model",
    "title": "Tobit Model",
    "section": "",
    "text": "To model these processes, we assume there is an underlying “Latent Variable” \\(y^*\\) that satisfies the classical linear assumptions:\n\\[\ny_i^* = x_i' \\beta + \\varepsilon_i\n\\]\nHowever, \\(y^*\\) is not perfectly observed.\n\n\n1. Censoring from Below (Left-Censoring)\nWe observe \\(y_i\\) based on a threshold \\(L\\):\n\\[\ny_i = \\begin{cases}\ny_i^* & \\text{if } y_i^* &gt; L \\\\\nL & \\text{if } y_i^* \\leq L\n\\end{cases}\n\\]\nResult: A “spike” (mass point) in the distribution of \\(y\\) at \\(L\\).\n2. Censoring from Above (Right-Censoring)\n\\[\ny_i = \\begin{cases}\ny_i^* & \\text{if } y_i^* &lt; U \\\\\nU & \\text{if } y_i^* \\geq U\n\\end{cases}\n\\]\n3. Truncation from Below\nWe only observe unit \\(i\\) if \\(y_i^* &gt; L\\).\n\\[\n\\text{Sample Rule: Keep } i \\text{ if } y_i^* &gt; L\n\\]\nResult: The distribution is “cut off.” The area under the curve must sum to 1, so the remaining probability density is scaled up."
  },
  {
    "objectID": "Notes/12censored_truncated/tobit.html#why-ols-fails-bias-analysis",
    "href": "Notes/12censored_truncated/tobit.html#why-ols-fails-bias-analysis",
    "title": "Tobit Model",
    "section": "",
    "text": "If we run a standard OLS regression (\\(y\\) on \\(x\\)) using censored data, our estimates will be biased.\n\n\nAt low values of \\(x\\), many observations hit the lower bound \\(L\\). This flattens the slope of the observed relationship relative to the true latent relationship.\n\n\n\n\n\n\n\n\n\n\n\n\nThe expected value of the observed data is:\n\\[\nE[y|x] = P(y^* \\le L) \\cdot L + P(y^* &gt; L) \\cdot E[y^* | y^* &gt; L, x]\n\\]\nThis relationship is non-linear.\n\nAt high \\(x\\), \\(E[y|x] \\approx E[y^*|x]\\) (convergence to true slope).\nAt low \\(x\\), data piles up at \\(L\\), dragging the expectation line flat.\nConsequence: OLS will generally underestimate the slope coefficient \\(\\beta\\)."
  },
  {
    "objectID": "Notes/12censored_truncated/tobit.html#estimation-strategies",
    "href": "Notes/12censored_truncated/tobit.html#estimation-strategies",
    "title": "Tobit Model",
    "section": "",
    "text": "Since OLS is inconsistent, we must use Maximum Likelihood Estimation (MLE). We need to make parametric assumptions (usually Normality) about the error term: \\(\\varepsilon \\sim N(0, \\sigma^2)\\).\n\n\nThe density of observed \\(y\\) is a mixture of a continuous part and a discrete probability mass.\n\nContinuous part (\\(y_i &gt; L\\)): We observe the actual value. The likelihood contribution is the PDF: \\(f^*(y_i | x_i)\\).\nCensored part (\\(y_i = L\\)): We know only that \\(y^* \\le L\\). The likelihood contribution is the probability (CDF): \\(F^*(L | x_i)\\).\n\nWe define a dummy indicator \\(d_i\\):\n\\[\nd_i = \\begin{cases}\n1 & \\text{if } y_i &gt; L \\quad (\\text{Uncensored}) \\\\\n0 & \\text{if } y_i = L \\quad (\\text{Censored})\n\\end{cases}\n\\]\nThe Log-Likelihood Function:\n\\[\n\\ell(\\beta, \\sigma^2) = \\sum_{i=1}^N \\left[ d_i \\ln \\left( \\frac{1}{\\sigma} \\Phi\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) \\right) + (1-d_i) \\ln \\Phi\\left(\\frac{L - x_i'\\beta}{\\sigma}\\right) \\right]\n\\]\n\n\n\nHere, we do not observe anyone with \\(y^* \\le L\\). We must re-normalize the probability density so it integrates to 1 over the range \\((L, \\infty)\\).\n\\[\nf(y_i | y_i &gt; L, x_i) = \\frac{f^*(y_i|x_i)}{P(y_i &gt; L | x_i)} = \\frac{\\frac{1}{\\sigma}\\phi(\\frac{y_i - x_i'\\beta}{\\sigma})}{1 - \\Phi(\\frac{L - x_i'\\beta}{\\sigma})}\n\\]\nThe Log-Likelihood Function:\n\\[\n\\ell = \\sum_{i=1}^N \\left[ \\ln \\left( \\frac{1}{\\sigma}\\Phi(\\cdot) \\right) - \\ln \\left( 1 - \\Phi\\left(\\frac{L - x_i'\\beta}{\\sigma}\\right) \\right) \\right]\n\\]\nIntuition: In the truncated case, the denominator \\(1 - \\Phi(\\cdot)\\) essentially tells the model: “Hey, we are missing the left tail of the bell curve, so inflate the probabilities of the data we do see to account for that missing mass.”"
  },
  {
    "objectID": "Notes/12censored_truncated/tobit.html#the-tobit-model-censored-normal-regression",
    "href": "Notes/12censored_truncated/tobit.html#the-tobit-model-censored-normal-regression",
    "title": "Tobit Model",
    "section": "",
    "text": "The Tobit model is the specific and most common case of censored regression where the threshold is zero (\\(L=0\\)) and censoring is from below.\n\n\n\nLatent Variable: \\(y_i^* = x_i'\\beta + \\varepsilon_i\\), with \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\).\nObservation Rule:\n\n\\[\ny_i = \\max(0, y_i^*)\n\\]\n\n\n\nA classic example is hours worked (\\(H_i\\)).\n\n\\(H_i^*\\): Desired hours (latent utility). A person might desire “-5 hours” of work (meaning they need a high wage just to start working).\n\\(H_i\\): Actual observed hours. Since you can’t work negative hours, we observe 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe split the sample into those working (\\(H_i &gt; 0\\)) and those not working (\\(H_i = 0\\)).\n\nFor \\(H_i &gt; 0\\) (Uncensored): Contribution is the density of the normal distribution:\n\n\\[\n\\frac{1}{\\sigma} \\Phi\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right)\n\\]\n\nFor \\(H_i = 0\\) (Censored): Contribution is the probability that latent utility is negative:\n\n\\[\n\\mathbb{P}(y_i^* \\le 0) = \\mathbb{P}\\left(\\frac{y_i^* - x_i'\\beta}{\\sigma} \\le \\frac{-x_i'\\beta}{\\sigma}\\right) = \\Phi\\left(-\\frac{x_i'\\beta}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\n\\]\nCombined Log-Likelihood:\n\\[\n\\ell(\\beta, \\sigma) = \\sum_{y_i = 0} \\ln \\left[ 1 - \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right) \\right] + \\sum_{y_i &gt; 0} \\ln \\left[ \\frac{1}{\\sigma} \\phi\\left(\\frac{y_i - x_i'\\beta}{\\sigma}\\right) \\right]\n\\]\n\n\n\n\nIdeally, we estimate \\(\\beta\\) and \\(\\sigma^2\\) separately.\nHowever, if there is high multicollinearity or little variation in the censored portion, it can be difficult to disentangle \\(\\beta\\) from \\(\\sigma\\).\nHomoskedasticity Assumption: Crucial for Tobit. If \\(\\text{Var}(\\varepsilon)\\) varies with \\(x\\), the Tobit estimates are inconsistent (unlike OLS, where heteroskedasticity only affects standard errors).\n\n\n\n\nIn Tobit, there are two different marginal effects of interest:\n\nEffect on the Latent Variable:\n\n\\[\n\\frac{\\partial E[y^*|x]}{\\partial x_k} = \\beta_k\n\\]\nInterpretation: How a change in \\(x\\) changes “desired” hours.\n\nEffect on the Observed Variable:\n\n\\[\n\\frac{\\partial E[y|x]}{\\partial x_k} = \\beta_k \\cdot \\Phi\\left(\\frac{x'\\beta}{\\sigma}\\right)\n\\]\nInterpretation: How a change in \\(x\\) changes actual hours worked.\nNote: The effect on actual hours is always smaller than the effect on desired hours because \\(0 \\le \\Phi(\\cdot) \\le 1\\). This dampening factor \\(\\Phi\\) represents the probability of being uncensored (actually working)."
  },
  {
    "objectID": "Problem_Sets/PS01/ps01.html",
    "href": "Problem_Sets/PS01/ps01.html",
    "title": "Problem Set 01",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Problem_Sets/PS03/ps03.html",
    "href": "Problem_Sets/PS03/ps03.html",
    "title": "Problem Set 03",
    "section": "",
    "text": "Back to top"
  }
]