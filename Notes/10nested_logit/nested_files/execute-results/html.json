{
  "hash": "bdab626fc8aceb21f0983f4844cefc87",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Nested Models\"\nauthor: \"Michael Fehl\"\ndescription: \"Nested Logit & Multinomial Probit\"\ndate: 28-01-2026\nimage: /images/nlogit_flowchart.png\nclass-notes: true\nformat:\n  html:\n    embed-resources: true\n---\n\n# Theory\n\n## Motivation\n\nRecall that from our multinomial models that they only hold under the IID alternatives assumption; this tells us that alternatives are completely independent of each other, each being drawn randomly with equal chance. Of course, we can think of immediate examples that would violate this assumption: cities such as Barcelona and Madrid are likely correlated; they share similar characteristics in language, culture, food, etc. Thus, deciding to move to one of these cities is surely influenced by each cities' similarities.\n\nIndirectly, this IID assumption leads to the \"**Independence of Irrelevant Alternatives**\" assumption, derived from the fact that relative probabilities for any two alternatives depend only on the attributes of those two alternatives. This implies that adding another alternative or changing the characteristics of a third does not affect the relative odds between any two alternatives $j$ and $\\ell$.\n\n## Nested Logit\n\nTo relax this assumption, we are going to allow\n\nTo understand the **Nested Logit** model, it's helpful to look at a diagram to understand the structure behind it. Let's imagine an example of how we decided to choose to pursue a Master's in Economics. The first decision you might have made is the <u>location</u>: will you study at a university that is *abroad* or *domestic*? This first decision are the two \"**limbs**\". From there, we make another, nested decision between the universities themselves: these final points of our tree are the *branches*. \n\n```{mermaid}\nflowchart TD\n  %% 1. Define a \"plain\" style: No border (stroke-width:0px), white background\n  classDef plain fill:#fff,stroke-width:0px,color:#000;\n\n  A{Masters in Economics} --> |Limb 1|B{Abroad}\n  A -->|Limb 2| C{Home}\n  B -->|Branch 1| D{BSE}\n  B -->|Branch 2| E{Carlos III}\n  C -->|Branch 1| F{UNC}\n  C -->|Branch 2| G{Duke}\n\n  %% 2. Create the \"Text\" nodes below using HTML for math\n  %% We use `---` to connect them, which creates a simple line\n  D --- D_text[\"V<sub>11</sub> + &epsilon;<sub>11</sub>\"]\n  E --- E_text[\"V<sub>12</sub> + &epsilon;<sub>12</sub>\"]\n  F --- F_text[\"V<sub>21</sub> + &epsilon;<sub>21</sub>\"]\n  G --- G_text[\"V<sub>22</sub> + &epsilon;<sub>22</sub>\"]\n\n  %% 3. Apply the \"plain\" style to your text nodes\n  class D_text,E_text,F_text,G_text plain\n```\n\n### Assumption: Error Distribution\n\nIn our previous multinomial model, we assumed that these branches within the same branches were uncorrelated; now, <u>we allow for same-branch limbs to be correlated</u>. However, it is important to note that there still is **no correlation across limbs**; i.e., $Corr(\\varepsilon_{11},\\varepsilon_{21}) = 0$. In our above example, this means that *BSE* and *Carlos III* are allow to be correlated in our model; i.e., $Corr(\\varepsilon_{11},\\varepsilon_{12})\\neq0$. This is a reasonable relaxation; we would expect both universities to share similar characteristics in language, professional network, grading scheme, etc.\n\nA major challenge with these types of nested models is the decision of structure itself; there are many ways we can construct this decision tree. For example, perhaps our first choice is based on cost; then on rankings, then faculty, then location... etc.\n\nTo relax this assumption of IID alternatives, rather than assuming $\\varepsilon_i \\sim \\mathcal{Gumbel}$ as we did before, we now assume that the errors follow a **General Extreme Value Distribution**; formally, $$\\varepsilon_i \\sim GEV()$$\n\nWhere the GEV CDF is written as the following: $$F(\\varepsilon) = \\exp \\left( - \\sum_{m=1}^{K} \\left( \\sum_{\\ell=1}^{K_m} e^{\\frac{-\\varepsilon_{m\\ell}}{\\rho_m}} \\right)^{\\rho_m} \\right)$$\n\n### Closed-Form Joint PDF\n\nPlugging this distribution into our joint pdf of the errors (integral), this simplifies into:\n\n$$ \nP_{jk}\n= \\Pr(y_i = jk \\mid X_i)\n= \n\\left[\n\\underbrace{\n\\frac{\\exp\\!\\left( V_{jk} / \\rho_m \\right)}\n{\\sum\\limits_{k \\in m} \\exp\\!\\left( V_{jk} / \\rho_m \\right)}\n}_{\\text{within-nest choice}}\n\\;\\times\\;\n\\underbrace{\n\\frac{\n\\left( \\sum\\limits_{k \\in m} \\exp\\!\\left( V_{jk} / \\rho_m \\right) \\right)^{\\rho_m}\n}{\n\\sum\\limits_{\\ell=1}^{M}\n\\left( \\sum\\limits_{k \\in \\ell} \\exp\\!\\left( V_{jk} / \\rho_\\ell \\right) \\right)^{\\rho_\\ell}\n}\n}_{\\text{nest choice}}\n\\right]\n$$\n\nWhere the *within-nest choice* is a logit model conditional on being in nest m, where we choose the best option available. The second term is the *nest choice*, where we decide the nest structure that has the best option inside each nest, on average.\n\nEssentially, we are decomposing the choice probability into these two components, seen below:\n\n### Choice Probability Decomposition\n\nSuppose there are $J$ limbs to choose from. The $j^{th}$ limb has $K_j$ branches: $j1, ..., jk,...,jK_j$\n\nThe utility for the alternative in the kth branch of the jth limb is:\n\n$$U_{jk} = V_{jk} + \\varepsilon_{jk}$$\nwhere $k = 1, 2, ..., K_j$ and $j = 1, 2, ..., J$. For a model with this nesting, $p_{jk}$ , the joint probability of being on limb $j$ and branch $k$ can be factored as $p_j$ , the probability of choosing limb $j$, times $p_{k \\mid j}$ , the probability of choosing branch $k$ conditional on being on limb $j$:\n\n$$\nP_{jk}=P_j × P_{k \\mid i}\n$$\n\nThis get rewritten as:\n\n$$\nP_{jk}\n=\n\\Pr(y_i = jk \\mid X_i)\n=\n\\left[\n\\frac{\ne^{V_{jk}}\n\\;\n\\frac{\\partial}{\\partial e^{V_{jk}}}\n\\left[\n\\sum_{m=1}^{K}\n\\left(\n\\sum_{\\ell =1}^{K_m}\n\\left(e^{V_{\\ell m}}\\right)^{1/\\rho_m}\n\\right)^{\\rho_m}\n\\right]\n}{\n\\sum_{m=1}^{K}\n\\left(\n\\sum_{\\ell = 1}^{K_m}\n\\left(e^{V_{\\ell m}}\\right)^{1/\\rho_m}\n\\right)^{\\rho_m}\n}\n\\right]\n$$\n\nWhere $\\rho_m$ is defined as our \"scale parameter\", measuring correlation b/w same-limb errors\n\n$$\n\\rho_m = \\sqrt{1-Corr(\\varepsilon_{m_\\ell},\\varepsilon_{mk})} \\in [0,1] \\\\\n\\rho_m = \\begin{cases}\n   1 &\\text{if no correlation b/w errors} \\\\\n   0 &\\text{if perfect correlation b/w errors} \n\\end{cases}\n$$\n\nNote that if $\\rho_m =1$, this formula collapses back down into the multinomial logit form from before. \n\n### Example\n\nSuppose we estimate the following model:\n\n$$V_{jk} = Z_j'\\alpha + X_{jk}'\\beta_j$$\n\nWhere $Z_j$ are our limb-specific regressors, i.e. varies over limbs *only*, and $X_{jk}$ varies both over *limbs* AND *branches*. $\\alpha, \\beta$ are the regression parameters.\n\nOur next step is to maximize $p_{jk} = p_j \\times p_{k \\mid j}$ w.r.t. $\\alpha, \\beta, \\text{and } \\rho$. Thus, we plug in our value for $V_{jk}$ into our $p_{jk}$ expression.\n\nThe joint distribution yields the nested logit model:\n\n$$\nP_{jk} = P_j \\times P_{k|j} = \\frac{\\exp(z'_j \\alpha + \\rho_j I_j)}{\\sum_{m=1}^{J} \\exp(z'_m \\alpha + \\rho_m I_m)} \\times \\frac{\\exp(x'_{jk} \\beta_j / \\rho_j)}{\\sum_{l=1}^{K_j} \\exp(x'_{jl} \\beta_j / \\rho_j)}\n$$\n\nwhere the so-called inclusive value $I_j$, is defined as:\n\n$$\nI_j = \\ln \\left( \\sum_{l=1}^{K_j} \\exp(x'_{jl} \\beta_j / \\rho_j) \\right)\n$$\n\n\nNote that this also works for regressors that do not vary over alternatives. In this case, $V_{jk} = z′\\alpha_j + x′\\beta_{jk}$, and we must normalize one of the $\\beta_{jk}$ for identification.\n\n### Estimation\n\nTo estimate the model parameters, we employ Maximum Likelihood Estimation (MLE).\n\n**1. Data Structure**\n\nLet $y_{ijk}$ be an indicator variable for the observed choice of the $i$-th individual.\n\n$$\ny_{ijk} =\n\\begin{cases}\n1 & \\text{if individual } i \\text{ chooses alternative } k \\text{ in nest } j \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\n**2. Joint Probability**\n\nThe probability of observing a specific choice is the product of the marginal probability of choosing the nest (limb) and the conditional probability of choosing the alternative (branch) within that nest:\n\n$$\np_{ijk} = p_{ij} \\times p_{ik|j}\n$$\n\n**3. The Likelihood Function**\n\nFor a single observation $y_i$, the probability mass function combines the choice probabilities with the observed indicator variables. Since $y_{ijk}=0$ for non-chosen alternatives (and $p^0=1$), only the chosen alternative contributes to the likelihood:\n\n$$\nf(y_i) = \\prod_{j=1}^{J} \\prod_{k=1}^{K_j} (p_{ij} \\times p_{ik|j})^{y_{ijk}}\n$$\n\n**4. The Log-Likelihood Function**\n\nWe maximize the log-likelihood function, $l(\\alpha, \\beta, \\rho)$. Taking the natural logarithm allows us to decompose the estimation into two additive components: the choice of the nest and the choice within the nest.\n\n$$\n\\ell(\\alpha, \\beta, \\rho) = \\sum_{i=1}^{N} \\left( \\underbrace{\\sum_{j=1}^{J} y_{ij} \\ln p_{ij}}_{\\text{Limb Choice}} + \\underbrace{\\sum_{j=1}^{J} \\sum_{k=1}^{K_j} y_{ijk} \\ln p_{ik|j}}_{\\text{Branch Choice}} \\right)\n$$\n\nThe solution to this maximization problem yields our estimators:\n\n$$\n(\\hat{\\alpha}, \\hat{\\beta}, \\hat{\\rho}) = \\operatorname*{argmax}_{\\alpha, \\beta, \\rho} \\ l(\\alpha, \\beta, \\rho)\n$$\n\n## Multinomial Probit\n\nSimilar motivations and logic as Nested Logit, with the only different being the form of the error distribution. As with regular probit regression, we assumed the errors to follow a normal distribution; more formally,\n\n$$\\varepsilon_i \\sim \\mathcal{N}(0, \\Sigma)$$\n\nWhere $\\varepsilon$ is a $(mx1)$ matrix, and \n\n$$\n\\Sigma = \n\\begin{bmatrix}\n  \\begin{bmatrix}\n    - & - \\\\\n    - & -\n  \\end{bmatrix} & 0 \\\\\n  0 & \\begin{bmatrix}\n    - & - \\\\\n    - & -\n  \\end{bmatrix}\n\\end{bmatrix}\n$$\n\nSuch that the variance of the errors allows for a nesting structure: errors amongst same-branch observations are allowed to be correlated (diagonal), but NOT across limbs (off-diagonal). This is very similar to the clustering standard errors from the heteroskedasticity relaxation from our random/fixed effects models.\n\n**Issue**: Multinomial Probit choice probability does <u>not</u> have a closed-form solution; for an $m$ choice problem, we have to solve for a $m-1$ fold integral; computationally very expensive as we move beyond 5 or so alternatives.\n\n# Applied: Stata\n\n```{stata nested logit}\nnlogit depvar [indepvars] [|| lev1 equation [|| lev2 equation ...]] || altvar: [byaltvarlist], case(varname)\n```\n\n\n```{stata multinomal probit}\nmprobit depvar [indepvars]\n```\n\n",
    "supporting": [
      "nested_files"
    ],
    "filters": [],
    "includes": {}
  }
}